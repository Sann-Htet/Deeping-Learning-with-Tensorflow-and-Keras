{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a259db59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-19 18:32:01.386806: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-19 18:32:01.467561: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-19 18:32:01.468599: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-19 18:32:03.232178: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "836142b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_read(url):\n",
    "    local_file = url.split('/')[-1]\n",
    "    local_file = local_file.replace(\"%20\", \" \")\n",
    "    p = tf.keras.utils.get_file(local_file, url, extract=True, cache_dir=\".\")\n",
    "    local_folder = os.path.join(\"datasets\", local_file.split('.')[0])\n",
    "    labeled_sentences = []\n",
    "    for labeled_filename in os.listdir(local_folder):\n",
    "        if labeled_filename.endswith(\"_labelled.txt\"):\n",
    "            with open(os.path.join(local_folder, labeled_filename), \"r\") as f:\n",
    "                for line in f:\n",
    "                    sentence, label = line.strip().split('\\t')\n",
    "                    labeled_sentences.append((sentence, label))\n",
    "    return labeled_sentences\n",
    "\n",
    "labeled_sentences = download_and_read(\n",
    "    \"https://archive.ics.uci.edu/ml/machine-learning-databases/00331/sentiment%20labelled%20sentences.zip\")\n",
    "sentences = [s for (s, l) in labeled_sentences]\n",
    "labels = [int(l) for (s, l) in labeled_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96a7e6ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size: 5271\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "vocab_size = len(tokenizer.word_counts)\n",
    "print(\"vocabulary size: {:d}\".format(vocab_size))\n",
    "word2idx = tokenizer.word_index\n",
    "idx2word = {v:k for (k, v) in word2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4fbbb4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(75, 16.0), (80, 18.0), (90, 22.0), (95, 26.0), (99, 36.0), (100, 71.0)]\n"
     ]
    }
   ],
   "source": [
    "seq_lengths = np.array([len(s.split()) for s in sentences])\n",
    "print([(p, np.percentile(seq_lengths, p)) for p in [75, 80, 90, 95, 99, 100]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f88f74dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-19 18:32:05.732706: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "max_seqlen = 64\n",
    "# create dataset\n",
    "sentences_as_ints = tokenizer.texts_to_sequences(sentences)\n",
    "sentences_as_ints = tf.keras.preprocessing.sequence.pad_sequences(sentences_as_ints, maxlen=max_seqlen)\n",
    "labels_as_ints = np.array(labels)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((sentences_as_ints, labels_as_ints))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a401650",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(10000)\n",
    "test_size = len(sentences) // 3\n",
    "val_size = (len(sentences) - test_size) // 10\n",
    "\n",
    "test_dataset = dataset.take(test_size)\n",
    "val_dataset = dataset.skip(test_size).take(val_size)\n",
    "train_dataset = dataset.skip(test_size + val_size)\n",
    "\n",
    "batch_size = 64\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "val_dataset = val_dataset.batch(batch_size)\n",
    "test_dataset = test_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b9d954f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalysisModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, max_seqlen, **kwargs):\n",
    "        super(SentimentAnalysisModel, self).__init__(**kwargs)\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size+1, max_seqlen)\n",
    "        self.bilstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(max_seqlen, return_sequences=False))\n",
    "        self.dense = tf.keras.layers.Dense(64, activation=\"relu\")\n",
    "        self.out = tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.bilstm(x)\n",
    "        x = self.dense(x)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84e1a39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sentiment_analysis_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  337408    \n",
      "                                                                 \n",
      " bidirectional (Bidirection  multiple                  66048     \n",
      " al)                                                             \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  8256      \n",
      "                                                                 \n",
      " dense_1 (Dense)             multiple                  65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 411777 (1.57 MB)\n",
      "Trainable params: 411777 (1.57 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = SentimentAnalysisModel(vocab_size, max_seqlen)\n",
    "model.build(input_shape=(batch_size, max_seqlen))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a487a5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile\n",
    "model.compile(optimizer=\"adam\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c97d4aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "29/29 [==============================] - 10s 79ms/step - loss: 0.6918 - accuracy: 0.5400 - val_loss: 0.6822 - val_accuracy: 0.7100\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 1s 28ms/step - loss: 0.6759 - accuracy: 0.7117 - val_loss: 0.5092 - val_accuracy: 0.7600\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 1s 29ms/step - loss: 0.4866 - accuracy: 0.8028 - val_loss: 0.4014 - val_accuracy: 0.8350\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 1s 28ms/step - loss: 0.3287 - accuracy: 0.8817 - val_loss: 0.2056 - val_accuracy: 0.9500\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 1s 40ms/step - loss: 0.2047 - accuracy: 0.9322 - val_loss: 0.1707 - val_accuracy: 0.9300\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 2s 54ms/step - loss: 0.1545 - accuracy: 0.9544 - val_loss: 0.1833 - val_accuracy: 0.9550\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 2s 57ms/step - loss: 0.1089 - accuracy: 0.9650 - val_loss: 0.0843 - val_accuracy: 0.9700\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 2s 55ms/step - loss: 0.0900 - accuracy: 0.9733 - val_loss: 0.0517 - val_accuracy: 0.9900\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 2s 56ms/step - loss: 0.0588 - accuracy: 0.9817 - val_loss: 0.0309 - val_accuracy: 0.9900\n",
      "Epoch 10/10\n",
      "29/29 [==============================] - 2s 54ms/step - loss: 0.0415 - accuracy: 0.9883 - val_loss: 0.0274 - val_accuracy: 0.9900\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "data_dir = \"./data\"\n",
    "logs_dir = os.path.join(\"./logs\")\n",
    "best_model_file = os.path.join(data_dir, \"best_model.h5\")\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(best_model_file,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 save_best_only=True)\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(log_dir=logs_dir)\n",
    "num_epochs = 10\n",
    "history = model.fit(train_dataset, epochs=num_epochs,\n",
    "                    validation_data=val_dataset,\n",
    "                    callbacks=[checkpoint, tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cff904ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae67189d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-c43f795f672fa75a\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-c43f795f672fa75a\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5431158e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate with test set\n",
    "best_model = SentimentAnalysisModel(vocab_size, max_seqlen)\n",
    "best_model.build(input_shape=(batch_size, max_seqlen))\n",
    "best_model.load_weights(best_model_file)\n",
    "best_model.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=\"adam\", \n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05800245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 3s 10ms/step - loss: 0.0321 - accuracy: 0.9920\n",
      "test loss: 0.032, test accuracy: 0.992\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = best_model.evaluate(test_dataset)\n",
    "print(\"test loss: {:.3f}, test accuracy: {:.3f}\".format(test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36fac530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 6ms/step\n",
      "1\t1\teverything about this film is simply incredible\n",
      "0\t0\tlike the other reviewer said you couldn't pay me to eat at this place again\n",
      "0\t0\tdo yourself a favor and stay away from this dish\n",
      "0\t0\ti as well would've given godfathers zero stars if possible\n",
      "0\t0\tthe only thing i wasn't too crazy about was their guacamole as i don't like it puréed\n",
      "1\t1\tvery convenient since we were staying at the mgm\n",
      "1\t1\tthis product is great it makes working a lot easier i can go to the copier while waiting on hold for something\n",
      "0\t0\ti don't know what happened in season five what a mess\n",
      "1\t1\tkieslowski never ceases to amaze me\n",
      "0\t0\tsprint charges for this service\n",
      "0\t0\ti have been in more than a few bars in vegas and do not ever recall being charged for tap water\n",
      "0\t0\tthe story starts too fast with absolutely no suspense or build up in the slightest\n",
      "1\t1\tfeelings thoughts gabriel's discomfort during the dance all these intangibles leap to life and come within the viewer's grasp in huston's portrayal\n",
      "0\t0\twhen my mom and i got home she immediately got sick and she only had a few bites of salad\n",
      "0\t0\tit wasn't busy either also the building was freezing cold\n",
      "1\t1\tthe entire audience applauded at the conclusion of the film\n",
      "1\t1\tin fact it's hard to remember that the part of ray charles is being acted and not played by the man himself\n",
      "0\t0\tthere were several moments in the movie that just didn't need to be there and were excruciatingly slow moving\n",
      "0\t0\tsadly gordon ramsey's steak is a place we shall sharply avoid during our next trip to vegas\n",
      "0\t0\ti was hoping for more\n",
      "0\t0\tthis place deserves one star and 90 has to do with the food\n",
      "1\t1\tof course the footage from the 70s was grainy but that only enhanced the film\n",
      "1\t1\twe made the drive all the way from north scottsdale and i was not one bit disappointed\n",
      "1\t1\tit was also the right balance of war and love\n",
      "0\t0\thow stupid is that\n",
      "0\t0\tthe fried rice was dry as well\n",
      "0\t0\tsoyo technology sucks\n",
      "1\t1\tat first glance it is a lovely bakery cafe nice ambiance clean friendly staff\n",
      "1\t1\ti loved their mussels cooked in this wine reduction the duck was tender and their potato dishes were delicious\n",
      "1\t1\ti am glad i purchased it\n",
      "1\t1\tall i have to say is the food was amazing\n",
      "0\t0\twhy are these sad little vegetables so overcooked\n",
      "0\t0\tall i can do is whine on the internet so here it goes the more i use the thing the less i like it\n",
      "0\t0\tlucy bell is so much higher than this crap and for her to sink this low is quite depressing\n",
      "0\t0\tthe live music on fridays totally blows\n",
      "0\t0\tone of the worst shows of all time\n",
      "1\t1\tthis item is fantastic and works perfectly\n",
      "0\t0\ti just got bored watching jessice lange take her clothes off\n",
      "0\t0\ti'm not really sure how joey's was voted best hot dog in the valley by readers of phoenix magazine\n",
      "1\t1\tour waiter was very attentive friendly and informative\n",
      "0\t0\tbut then they came back cold\n",
      "1\t1\thighly recommended\n",
      "0\t0\tshe was quite disappointed although some blame needs to be placed at her door\n",
      "1\t1\ttechnically the film is well made with impressive camera work solid acting and effective music from riz ortolani  particularly good is a recurring unaccompanied female vocal that sounds like it's coming from a distant hill\n",
      "1\t1\tthe scripting of the subtle comedy is unmatched by any movie in recent years\n",
      "1\t1\ti wear it everyday and it holds up very well\n",
      "0\t0\tthe story line is totally predictable\n",
      "0\t0\ti've bought 5 wired headphones that sound better than these\n",
      "1\t1\tthe only thing really worth watching was the scenery and the house because it is beautiful\n",
      "0\t0\ti dont think i will be back for a very long time\n",
      "1\t1\tmuch more interesting more action more suspense and less of the unneeded controversy\n",
      "0\t0\tthe only thing worse than taylor's acting was stanwyck's singing\n",
      "1\t1\ti usually don't like headbands but this one is very lightweight doesn't mess up my hair\n",
      "1\t1\tit has everything i need and i couldn't ask for more\n",
      "1\t1\tmy boyfriend and i came here for the first time on a recent trip to vegas and could not have been more pleased with the quality of food and service\n",
      "1\t1\ti love my 350 headset my jabra350 bluetooth headset is great the reception is very good and the ear piece is a comfortable fit\n",
      "1\t1\tthe staff was very attentive\n",
      "0\t0\tadapter does not provide enough charging current\n",
      "0\t0\tit's a mediocre miserable hollow laughable and predictable piece of garbage\n",
      "0\t0\thost staff were for lack of a better word bitches\n",
      "1\t1\tcheck it out\n",
      "0\t0\taside from it's terrible lead this film has loads of other debits\n",
      "1\t1\tsetup couldn't have been simpler\n",
      "1\t1\tgreat food for the price which is very high quality and house made\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 12ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 1s 4ms/step\n",
      "accuracy score: 0.993\n",
      "confusion matrix\n",
      "[[490   2]\n",
      " [  5 503]]\n"
     ]
    }
   ],
   "source": [
    "# predict on batches\n",
    "labels, predictions = [], []\n",
    "idx2word[0] = \"PAD\"\n",
    "is_first_batch = True\n",
    "for test_batch in test_dataset:\n",
    "    inputs_b, labels_b = test_batch\n",
    "    pred_batch = best_model.predict(inputs_b)\n",
    "    predictions.extend([(1 if p > 0.5 else 0) for p in pred_batch])\n",
    "    labels.extend([l for l in labels_b])\n",
    "    if is_first_batch:\n",
    "        for rid in range(inputs_b.shape[0]):\n",
    "            words = [idx2word[idx] for idx in inputs_b[rid].numpy()]\n",
    "            words = [w for w in words if w != \"PAD\"]\n",
    "            sentence = \" \".join(words)\n",
    "            print(\"{:d}\\t{:d}\\t{:s}\".format(labels[rid], predictions[rid], sentence))\n",
    "        is_first_batch = False\n",
    "\n",
    "print(\"accuracy score: {:.3f}\".format(accuracy_score(labels, predictions)))\n",
    "print(\"confusion matrix\")\n",
    "print(confusion_matrix(labels, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e55695",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
