{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d50e24b",
   "metadata": {},
   "source": [
    "Let’s verify that everything is working correctly by downloading a pretrained model used\n",
    "for sentiment analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85b01756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-26 17:45:29.880502: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-26 17:45:30.056772: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-26 17:45:30.057796: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-26 17:45:31.177511: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Downloading (…)lve/main/config.json: 100%|█████| 629/629 [00:00<00:00, 1.48MB/s]\n",
      "Downloading model.safetensors: 100%|██████████| 268M/268M [05:00<00:00, 890kB/s]\n",
      "2023-08-26 17:50:35.195439: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-08-26 17:50:35.283876: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 93763584 exceeds 10% of free system memory.\n",
      "2023-08-26 17:50:35.382443: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 93763584 exceeds 10% of free system memory.\n",
      "2023-08-26 17:50:35.395074: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 93763584 exceeds 10% of free system memory.\n",
      "2023-08-26 17:50:35.973784: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 93763584 exceeds 10% of free system memory.\n",
      "2023-08-26 17:50:36.012568: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 93763584 exceeds 10% of free system memory.\n",
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n",
      "Downloading (…)okenizer_config.json: 100%|███| 48.0/48.0 [00:00<00:00, 89.3kB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|████| 232k/232k [00:00<00:00, 895kB/s]\n",
      "[{'label': 'POSITIVE', 'score': 0.9998704195022583}]\n"
     ]
    }
   ],
   "source": [
    "!python -c \"from transformers import pipeline; print(pipeline('sentiment-analysis')('we love you'))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f88c8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sann-htet/anaconda3/envs/transformer/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-08-26 17:50:40.888484: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-26 17:50:40.928667: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-26 17:50:40.929306: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-26 17:50:41.830527: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "No model was supplied, defaulted to gpt2 and revision 6c0e608 (https://huggingface.co/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Downloading (…)lve/main/config.json: 100%|█████| 665/665 [00:00<00:00, 1.05MB/s]\n",
      "Downloading model.safetensors: 100%|██████████| 548M/548M [10:16<00:00, 889kB/s]\n",
      "2023-08-26 18:01:01.142470: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-08-26 18:01:01.257179: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 154389504 exceeds 10% of free system memory.\n",
      "2023-08-26 18:01:01.413390: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 154389504 exceeds 10% of free system memory.\n",
      "2023-08-26 18:01:01.443833: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 154389504 exceeds 10% of free system memory.\n",
      "2023-08-26 18:01:02.365440: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 154389504 exceeds 10% of free system memory.\n",
      "2023-08-26 18:01:02.624515: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 154389504 exceeds 10% of free system memory.\n",
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "Downloading (…)olve/main/vocab.json: 100%|█| 1.04M/1.04M [00:00<00:00, 1.23MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|███| 456k/456k [00:00<00:00, 1.19MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██| 1.36M/1.36M [00:01<00:00, 937kB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "generator = pipeline(task=\"text-generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68fe1511",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone. Or, perhaps, to the Elven-kings, and the Dwarves and Dwarves beneath them.\\nAnd so I began'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d240ddc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'The original theory of relativity is based upon the premisethat all coordinate systems in relative uniform translatory motion toeach other are equally valid and equivalent, yet is still not in our knowledge. If any new theory is ever conceived. What does an'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator (\"The original theory of relativity is based upon the premise\" + \n",
    "           \"that all coordinate systems in relative uniform translatory motion to\" + \n",
    "           \"each other are equally valid and equivalent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78d5c0e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'It takes a great deal of bravery to stand up to our enemies,\" Kuchera said. \"If our enemies know who our allies are, they will use the very tactic to help their enemies, that is why we have seen strong leaders here'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator (\"It takes a great deal of bravery to stand up to our enemies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f84f38",
   "metadata": {},
   "source": [
    "## Autoselecting a model and autotokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b63ef2",
   "metadata": {},
   "source": [
    "We can easily import a pretrained model among the several dozen available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "433202db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|█████| 483/483 [00:00<00:00, 1.42MB/s]\n",
      "Downloading model.safetensors: 100%|██████████| 268M/268M [05:00<00:00, 891kB/s]\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e228710",
   "metadata": {},
   "source": [
    "We can use AutoTokenizer to transform words into tokens used by the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74c052de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|███| 28.0/28.0 [00:00<00:00, 65.7kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|█████| 570/570 [00:00<00:00, 1.37MB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|████| 232k/232k [00:00<00:00, 808kB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|███| 466k/466k [00:00<00:00, 2.20MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 1996, 2434, 3399, 1997, 20805, 2003, 2241, 2588, 1996, 18458, 2008, 2035, 13530, 3001, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "sequence = \"The original theory of relativity is based upon the premise that all coordinate systems\"\n",
    "print(tokenizer(sequence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16657e62",
   "metadata": {},
   "source": [
    "## Named entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f8c39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Downloading (…)lve/main/config.json: 100%|█████| 998/998 [00:00<00:00, 1.85MB/s]\n",
      "Downloading tf_model.h5:  84%|███████████▊  | 1.12G/1.33G [22:52<04:45, 746kB/s]"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "ner_pipe = pipeline(\"ner\")\n",
    "sequence = \"\"\"Mr. and Mrs. Dursley, of number four, Privet Drive, were \n",
    "              proud to say that they were perfectly normal, thank you very much.\"\"\"\n",
    "for entity in ner_pipe(sequence):\n",
    "    print(entity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281cfe72",
   "metadata": {},
   "source": [
    "## Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab06621d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "summarizer = pipeline(\"summarization\")\n",
    "ARTICLE = \"\"\"\n",
    "Mr.\n",
    "and Mrs.\n",
    "Dursley, of number four, Privet Drive, were proud to say\n",
    "that they were perfectly normal, thank you very much.\n",
    "They were the last\n",
    "people you'd expect to be involved in anything strange or mysterious,\n",
    "because they just didn't hold with such nonsense.\n",
    "Mr.\n",
    "Dursley was the director of a firm called Grunnings, which made\n",
    "drills.\n",
    "He was a big, beefy man with hardly any neck, although he did\n",
    "have a very large mustache.\n",
    "Mrs.\n",
    "Dursley was thin and blonde and had\n",
    "nearly twice the usual amount of neck, which came in very useful as she\n",
    "spent so much of her time craning over garden fences, spying on the\n",
    "neighbors.\n",
    "The Dursleys had a small son called Dudley and in their\n",
    "opinion there was no finer boy anywhere\"\"\"\n",
    "print(summarizer(ARTICLE, max_length=130, min_length=30, do_sample=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b75b74",
   "metadata": {},
   "source": [
    "If we want to change to a different model. That’s extremely simple as we only\n",
    "need to change one parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc5ad24",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = pipeline(\"summarization\", model='t5-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa167e4a",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e15d55",
   "metadata": {},
   "source": [
    "First, let’s load and tokenize the Yelp dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d2a85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"yelp_review_full\")\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823470dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_train_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606cd35c",
   "metadata": {},
   "source": [
    "Then let’s convert them to TF format datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943adb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DefaultDataCollator\n",
    "data_collator = DefaultDataCollator(return_tensorfs=\"tf\")\n",
    "\n",
    "# convert the tokenized datasets to TensorFlow datasets\n",
    "tf_train_dataset = small_train_dataset.to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n",
    "    label_cols=[\"labels\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "\n",
    "tf_validation_dataset = small_eval_dataset.to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n",
    "    label_cols=[\"labels\"],\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f003153",
   "metadata": {},
   "source": [
    "Now, we can use `TFAutoModelForSequenceClassification`, specifically selecting **bert-base-\n",
    "cased**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed75a087",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6eef4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=tf.metrics.SparseCategoricalAccuracy(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef39539",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(tf_train_dataset, \n",
    "          validation_data=tf_validation_dataset,\n",
    "          epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff63e090",
   "metadata": {},
   "source": [
    "## TFHub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ca2eb9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
