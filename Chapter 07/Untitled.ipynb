{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "324ef84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-28 18:03:32.849890: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-28 18:03:32.932556: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-28 18:03:32.933586: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-28 18:03:34.600087: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83aa274b",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, _), (test_data, _) =  tf.keras.datasets.mnist.load_data()\n",
    "train_data = train_data/np.float32(255)\n",
    "train_data = np.reshape(train_data, (train_data.shape[0], 784))\n",
    "\n",
    "\n",
    "test_data = test_data/np.float32(255)\n",
    "test_data = np.reshape(test_data, (test_data.shape[0], 784))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f056ea5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class that defines the behavior of the RBM\n",
    "class RBM(object):\n",
    "    \n",
    "    def __init__(self, input_size, output_size, lr=1.0, batchsize=100):\n",
    "        \"\"\"\n",
    "        m: Number of neurons in visible layer\n",
    "        n: number of neurons in hidden layer\n",
    "        \"\"\"\n",
    "        #Defining the hyperparameters\n",
    "        self._input_size = input_size #Size of Visible\n",
    "        self._output_size = output_size #Size of outp\n",
    "        self.learning_rate = lr #The step used in gradient descent\n",
    "        self.batchsize = batchsize #The size of how much data will be used for training per sub iteration\n",
    "        \n",
    "        #Initializing weights and biases as matrices full of zeroes\n",
    "        self.w = tf.zeros([input_size, output_size], np.float32) #Creates and initializes the weights with 0\n",
    "        self.hb = tf.zeros([output_size], np.float32) #Creates and initializes the hidden biases with 0\n",
    "        self.vb = tf.zeros([input_size], np.float32) #Creates and initializes the visible biases with 0\n",
    "\n",
    "\n",
    "    #Forward Pass\n",
    "    def prob_h_given_v(self, visible, w, hb):\n",
    "        #Sigmoid \n",
    "        return tf.nn.sigmoid(tf.matmul(visible, w) + hb)\n",
    "\n",
    "    #Backward Pass\n",
    "    def prob_v_given_h(self, hidden, w, vb):\n",
    "        return tf.nn.sigmoid(tf.matmul(hidden, tf.transpose(w)) + vb)\n",
    "    \n",
    "    #Generate the sample probability\n",
    "    def sample_prob(self, probs):\n",
    "        return tf.nn.relu(tf.sign(probs - tf.random.uniform(tf.shape(probs))))\n",
    "\n",
    "    #Training method for the model\n",
    "    def train(self, X, epochs=10):\n",
    "               \n",
    "        loss = []\n",
    "        for epoch in range(epochs):\n",
    "            #For each step/batch\n",
    "            for start, end in zip(range(0, len(X), self.batchsize),range(self.batchsize,len(X), self.batchsize)):\n",
    "                batch = X[start:end]\n",
    "                    \n",
    "                #Initialize with sample probabilities\n",
    "                    \n",
    "                h0 = self.sample_prob(self.prob_h_given_v(batch, self.w, self.hb))\n",
    "                v1 = self.sample_prob(self.prob_v_given_h(h0, self.w, self.vb))\n",
    "                h1 = self.prob_h_given_v(v1, self.w, self.hb)\n",
    "                    \n",
    "                #Create the Gradients\n",
    "                positive_grad = tf.matmul(tf.transpose(batch), h0)\n",
    "                negative_grad = tf.matmul(tf.transpose(v1), h1)\n",
    "                    \n",
    "                #Update learning rates \n",
    "                self.w = self.w + self.learning_rate *(positive_grad - negative_grad) / tf.dtypes.cast(tf.shape(batch)[0],tf.float32)\n",
    "                self.vb = self.vb +  self.learning_rate * tf.reduce_mean(batch - v1, 0)\n",
    "                self.hb = self.hb +  self.learning_rate * tf.reduce_mean(h0 - h1, 0)\n",
    "                    \n",
    "            #Find the error rate\n",
    "            err = tf.reduce_mean(tf.square(batch - v1))\n",
    "            print ('Epoch: %d' % epoch,'reconstruction error: %f' % err)\n",
    "            loss.append(err)\n",
    "                    \n",
    "        return loss\n",
    "        \n",
    "    #Create expected output for our DBN\n",
    "    def rbm_output(self, X):\n",
    "        out = tf.nn.sigmoid(tf.matmul(X, self.w) + self.hb)\n",
    "        return out\n",
    "    \n",
    "    def rbm_reconstruct(self,X):\n",
    "        h = tf.nn.sigmoid(tf.matmul(X, self.w) + self.hb)\n",
    "        reconstruct = tf.nn.sigmoid(tf.matmul(h, tf.transpose(self.w)) + self.vb)\n",
    "        return reconstruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf1014b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESHAPED = 784\n",
    "NB_CLASSES = 10   # number of outputs = number of digits\n",
    "\n",
    "(train_data, Y_train), (test_data, Y_test) =  tf.keras.datasets.mnist.load_data()\n",
    "train_data = train_data/np.float32(255)\n",
    "train_data = np.reshape(train_data, (train_data.shape[0], RESHAPED))\n",
    "Y_train = tf.keras.utils.to_categorical(Y_train, NB_CLASSES)\n",
    "Y_test = tf.keras.utils.to_categorical(Y_test, NB_CLASSES)\n",
    "\n",
    "test_data = test_data/np.float32(255)\n",
    "test_data = np.reshape(test_data, (test_data.shape[0], RESHAPED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21cc70a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RBM:  0   784 -> 500\n",
      "RBM:  1   500 -> 200\n",
      "RBM:  2   200 -> 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-28 18:04:10.017476: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "RBM_hidden_sizes = [500, 200 , 50 ] #create 2 layers of RBM with size 400 and 100\n",
    "\n",
    "#Since we are training, set input as training data\n",
    "inpX = train_data\n",
    "\n",
    "#Create list to hold our RBMs\n",
    "rbm_list = []\n",
    "\n",
    "#Size of inputs is the number of inputs in the training set\n",
    "input_size = train_data.shape[1]\n",
    "\n",
    "#For each RBM we want to generate\n",
    "for i, size in enumerate(RBM_hidden_sizes):\n",
    "    print ('RBM: ',i,' ',input_size,'->', size)\n",
    "    rbm_list.append(RBM(input_size, size))\n",
    "    input_size = size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1acf2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next RBM:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-28 18:04:20.113944: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 188160000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 reconstruction error: 0.057256\n",
      "Epoch: 1 reconstruction error: 0.052452\n"
     ]
    }
   ],
   "source": [
    "#For each RBM in our list\n",
    "for rbm in rbm_list:\n",
    "    print ('Next RBM:')\n",
    "    #Train a new one\n",
    "    rbm.train(tf.cast(inpX,tf.float32)) \n",
    "    #Return the output layer\n",
    "    inpX = rbm.rbm_output(inpX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316ad1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = rbm_list[0].rbm_reconstruct(test_data)\n",
    "# Plotting original and reconstructed images\n",
    "row, col = 2, 8\n",
    "idx = np.random.randint(0, 100, row * col // 2)\n",
    "f, axarr = plt.subplots(row, col, sharex=True, sharey=True, figsize=(20,4))\n",
    "for fig, row in zip([test_data,out], axarr):\n",
    "    for i,ax in zip(idx,row):\n",
    "        ax.imshow(tf.reshape(fig[i],[28, 28]), cmap='Greys_r')\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3634f80b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
