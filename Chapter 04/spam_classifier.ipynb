{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a6d8e86",
   "metadata": {},
   "source": [
    "# Using word embeddings for spam detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b4c9619",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-16 13:29:37.260261: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-16 13:29:37.332115: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-16 13:29:37.332985: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-16 13:29:38.849132: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4688e0a1",
   "metadata": {},
   "source": [
    "## Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "962d82f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_read(url):\n",
    "    local_file = url.split('/')[-1]\n",
    "    p = tf.keras.utils.get_file(local_file, url, extract=True, cache_dir='.')\n",
    "    labels, texts = [], []\n",
    "    local_file = os.path.join(\"datasets\", \"SMSSpamCollection\")\n",
    "    with open(local_file, \"r\") as fin:\n",
    "        for line in fin:\n",
    "            label, text = line.strip().split('\\t')\n",
    "            labels.append(1 if label == \"spam\" else 0)\n",
    "            texts.append(text)\n",
    "    return texts, labels\n",
    "    \n",
    "DATASET_URL = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\"\n",
    "texts, labels = download_and_read(DATASET_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c55d85",
   "metadata": {},
   "source": [
    "## Making the data ready for use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56068e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5574 sentences, max length: 189\n"
     ]
    }
   ],
   "source": [
    "# tokenize and pad text\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "text_sequences = tokenizer.texts_to_sequences(texts)\n",
    "text_sequences = tf.keras.preprocessing.sequence.pad_sequences(text_sequences)\n",
    "num_records = len(text_sequences)\n",
    "max_seqlen = len(text_sequences[0])\n",
    "print(\"{:d} sentences, max length: {:d}\".format(num_records, max_seqlen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98767206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels\n",
    "NUM_CLASSES = 2\n",
    "cat_labels = tf.keras.utils.to_categorical(labels, num_classes=NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ca9bc49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 9010\n"
     ]
    }
   ],
   "source": [
    "# vocabulary\n",
    "word2idx = tokenizer.word_index\n",
    "idx2word = {v:k for k, v in word2idx.items()}\n",
    "word2idx[\"PAD\"] = 0\n",
    "idx2word[0] = \"PAD\"\n",
    "vocab_size = len(word2idx)\n",
    "print(\"vocab size: {:d}\".format(vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "155a1738",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-16 13:29:40.638525: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "# dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((text_sequences, cat_labels))\n",
    "dataset = dataset.shuffle(10000)\n",
    "test_size = num_records // 4\n",
    "val_size = (num_records - test_size) // 10\n",
    "test_dataset = dataset.take(test_size)\n",
    "val_dataset = dataset.skip(test_size).take(val_size)\n",
    "train_dataset = dataset.skip(test_size + val_size)\n",
    "BATCH_SIZE = 128\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc794613",
   "metadata": {},
   "source": [
    "## Building the embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8039061c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b141e90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def building_embedding_matrix(sequences, word2idx, embedding_dim, embedding_file):\n",
    "    if os.path.exists(embedding_file):\n",
    "        E = np.load(embedding_file)\n",
    "    else:\n",
    "        vocab_size = len(word2idx)\n",
    "        E = np.zeros((vocab_size, embedding_dim))\n",
    "        word_vectors = api.load(EMBEDDING_MODEL)\n",
    "        for word, idx in word2idx.items():\n",
    "            try:\n",
    "                E[idx] = word_vectors.word_vec(word)\n",
    "            except KeyError: # word not in embedding\n",
    "                pass\n",
    "        np.save(embedding_file, E)\n",
    "    return E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f09acde0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 376.1/376.1MB downloaded\n",
      "Embedding matrix: (9010, 300)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16052/1224366059.py:10: DeprecationWarning: Call to deprecated `word_vec` (Use get_vector instead).\n",
      "  E[idx] = word_vectors.word_vec(word)\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "DATA_DIR = \"data\"\n",
    "EMBEDDING_NUMPY_FILE = os.path.join(DATA_DIR, \"E.npy\")\n",
    "EMBEDDING_MODEL = \"glove-wiki-gigaword-300\"\n",
    "E = building_embedding_matrix(text_sequences, word2idx, EMBEDDING_DIM, EMBEDDING_NUMPY_FILE)\n",
    "print(\"Embedding matrix:\", E.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c7640425",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpamClassifierModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_sz, embed_sz, input_length,\n",
    "            num_filters, kernel_sz, output_sz, \n",
    "            run_mode, embedding_weights, \n",
    "            **kwargs):\n",
    "        super(SpamClassifierModel, self).__init__(**kwargs)\n",
    "        if run_mode == \"scratch\":\n",
    "            self.embedding = tf.keras.layers.Embedding(vocab_sz, \n",
    "                embed_sz,\n",
    "                input_length=input_length,\n",
    "                trainable=True)\n",
    "        elif run_mode == \"vectorizer\":\n",
    "            self.embedding = tf.keras.layers.Embedding(vocab_sz, \n",
    "                embed_sz,\n",
    "                input_length=input_length,\n",
    "                weights=[embedding_weights],\n",
    "                trainable=False)\n",
    "        else:\n",
    "            self.embedding = tf.keras.layers.Embedding(vocab_sz, \n",
    "                embed_sz,\n",
    "                input_length=input_length,\n",
    "                weights=[embedding_weights],\n",
    "                trainable=True)\n",
    "        self.dropout = tf.keras.layers.SpatialDropout1D(0.2)\n",
    "        self.conv = tf.keras.layers.Conv1D(filters=num_filters,\n",
    "            kernel_size=kernel_sz,\n",
    "            activation=\"relu\")\n",
    "        self.pool = tf.keras.layers.GlobalMaxPooling1D()\n",
    "        self.dense = tf.keras.layers.Dense(output_sz, \n",
    "            activation=\"softmax\"\n",
    "        )\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.dense(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4a6bd3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model definition\n",
    "conv_num_filters = 256\n",
    "conv_kernel_size = 3\n",
    "run_mode = \"vectorizer\"\n",
    "model = SpamClassifierModel(\n",
    "    vocab_size, EMBEDDING_DIM, max_seqlen, \n",
    "    conv_num_filters, conv_kernel_size, NUM_CLASSES,\n",
    "    run_mode, E)\n",
    "model.build(input_shape=(None, max_seqlen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b274e9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile\n",
    "model.compile(optimizer=\"adam\",\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bffa9606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"spam_classifier_model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     multiple                  2703000   \n",
      "                                                                 \n",
      " spatial_dropout1d_1 (Spati  multiple                  0         \n",
      " alDropout1D)                                                    \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           multiple                  230656    \n",
      "                                                                 \n",
      " global_max_pooling1d_1 (Gl  multiple                  0         \n",
      " obalMaxPooling1D)                                               \n",
      "                                                                 \n",
      " dense_1 (Dense)             multiple                  514       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2934170 (11.19 MB)\n",
      "Trainable params: 231170 (903.01 KB)\n",
      "Non-trainable params: 2703000 (10.31 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "148460e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 3\n",
    "# data distribution is 4827 ham and 747 spam (total 5574), which\n",
    "# works out to approx 87% ham and 13% spam, so we take reciprocals\n",
    "# and this works out to being each spam (1) item as being\n",
    "# approximately 8 times as important as each ham (0) message.\n",
    "CLASS_WEIGHTS = {0:1, 1:8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "84305ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "29/29 [==============================] - 7s 190ms/step - loss: 0.5390 - accuracy: 0.8623 - val_loss: 0.1374 - val_accuracy: 0.9531\n",
      "Epoch 2/3\n",
      "29/29 [==============================] - 5s 181ms/step - loss: 0.2127 - accuracy: 0.9582 - val_loss: 0.0679 - val_accuracy: 0.9792\n",
      "Epoch 3/3\n",
      "29/29 [==============================] - 5s 180ms/step - loss: 0.1524 - accuracy: 0.9747 - val_loss: 0.0650 - val_accuracy: 0.9844\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f1096f86ad0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train model\n",
    "model.fit(train_dataset, epochs=NUM_EPOCHS,\n",
    "          validation_data=val_dataset,\n",
    "          class_weight=CLASS_WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "45ec76f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 0.983\n",
      "confusion matrix\n",
      "[[1097   21]\n",
      " [   1  161]]\n"
     ]
    }
   ],
   "source": [
    "# evaluate against test set\n",
    "labels, predictions = [], []\n",
    "for Xtest, Ytest in test_dataset:\n",
    "    Ytest_ = model.predict_on_batch(Xtest)\n",
    "    ytest = np.argmax(Ytest, axis=1)\n",
    "    ytest_ = np.argmax(Ytest_, axis=1)\n",
    "    labels.extend(ytest.tolist())\n",
    "    predictions.extend(ytest_.tolist())\n",
    "\n",
    "print(\"test accuracy: {:.3f}\".format(accuracy_score(labels, predictions)))\n",
    "print(\"confusion matrix\")\n",
    "print(confusion_matrix(labels, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8265ca",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
