{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84b897df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image     #This is used for rendering images in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3ab84ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-31 17:52:25.572909: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-31 17:52:25.657936: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-31 17:52:25.658935: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-31 17:52:27.570441: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "[nltk_data] Downloading package punkt to /home/sann-htet/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package reuters to /home/sann-\n",
      "[nltk_data]     htet/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import RepeatVector\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from scipy.stats import describe\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "from time import gmtime, strftime\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import re\n",
    "# Needed to run only once\n",
    "nltk.download('punkt')\n",
    "nltk.download('reuters')\n",
    "from nltk.corpus import reuters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13626a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!unzip /root/nltk_data/corpora/reuters.zip -d /root/nltk_data/corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c94a864",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "#!unzip glove*.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2687402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building features from 0 docs\n",
      "building features from 100 docs\n",
      "building features from 200 docs\n",
      "building features from 300 docs\n",
      "building features from 400 docs\n",
      "building features from 500 docs\n",
      "building features from 600 docs\n",
      "building features from 700 docs\n",
      "building features from 800 docs\n",
      "building features from 900 docs\n",
      "building features from 1000 docs\n",
      "building features from 1100 docs\n",
      "building features from 1200 docs\n",
      "building features from 1300 docs\n",
      "building features from 1400 docs\n",
      "building features from 1500 docs\n",
      "building features from 1600 docs\n",
      "building features from 1700 docs\n",
      "building features from 1800 docs\n",
      "building features from 1900 docs\n",
      "building features from 2000 docs\n",
      "building features from 2100 docs\n",
      "building features from 2200 docs\n",
      "building features from 2300 docs\n",
      "building features from 2400 docs\n",
      "building features from 2500 docs\n",
      "building features from 2600 docs\n",
      "building features from 2700 docs\n",
      "building features from 2800 docs\n",
      "building features from 2900 docs\n",
      "building features from 3000 docs\n",
      "building features from 3100 docs\n",
      "building features from 3200 docs\n",
      "building features from 3300 docs\n",
      "building features from 3400 docs\n",
      "building features from 3500 docs\n",
      "building features from 3600 docs\n",
      "building features from 3700 docs\n",
      "building features from 3800 docs\n",
      "building features from 3900 docs\n",
      "building features from 4000 docs\n",
      "building features from 4100 docs\n",
      "building features from 4200 docs\n",
      "building features from 4300 docs\n",
      "building features from 4400 docs\n",
      "building features from 4500 docs\n",
      "building features from 4600 docs\n",
      "building features from 4700 docs\n",
      "building features from 4800 docs\n",
      "building features from 4900 docs\n",
      "building features from 5000 docs\n",
      "building features from 5100 docs\n",
      "building features from 5200 docs\n",
      "building features from 5300 docs\n",
      "building features from 5400 docs\n",
      "building features from 5500 docs\n",
      "building features from 5600 docs\n",
      "building features from 5700 docs\n",
      "building features from 5800 docs\n",
      "building features from 5900 docs\n",
      "building features from 6000 docs\n",
      "building features from 6100 docs\n",
      "building features from 6200 docs\n",
      "building features from 6300 docs\n",
      "building features from 6400 docs\n",
      "building features from 6500 docs\n",
      "building features from 6600 docs\n",
      "building features from 6700 docs\n",
      "building features from 6800 docs\n",
      "building features from 6900 docs\n",
      "building features from 7000 docs\n",
      "building features from 7100 docs\n",
      "building features from 7200 docs\n",
      "building features from 7300 docs\n",
      "building features from 7400 docs\n",
      "building features from 7500 docs\n",
      "building features from 7600 docs\n",
      "building features from 7700 docs\n",
      "building features from 7800 docs\n",
      "building features from 7900 docs\n",
      "building features from 8000 docs\n",
      "building features from 8100 docs\n",
      "building features from 8200 docs\n",
      "building features from 8300 docs\n",
      "building features from 8400 docs\n",
      "building features from 8500 docs\n",
      "building features from 8600 docs\n",
      "building features from 8700 docs\n",
      "building features from 8800 docs\n",
      "building features from 8900 docs\n",
      "building features from 9000 docs\n",
      "building features from 9100 docs\n",
      "building features from 9200 docs\n",
      "building features from 9300 docs\n",
      "building features from 9400 docs\n",
      "building features from 9500 docs\n",
      "building features from 9600 docs\n",
      "building features from 9700 docs\n",
      "building features from 9800 docs\n",
      "building features from 9900 docs\n",
      "building features from 10000 docs\n",
      "building features from 10100 docs\n",
      "building features from 10200 docs\n",
      "building features from 10300 docs\n",
      "building features from 10400 docs\n",
      "building features from 10500 docs\n",
      "building features from 10600 docs\n",
      "building features from 10700 docs\n"
     ]
    }
   ],
   "source": [
    "def is_number(n):\n",
    "    temp = re.sub(\"[.,-/]\", \"\", n)\n",
    "    return temp.isdigit()\n",
    "\n",
    "# parsing sentences and building vocabulary\n",
    "word_freqs = collections.Counter()\n",
    "documents = reuters.fileids()\n",
    "#ftext = open(\"text.tsv\", \"r\")\n",
    "sents = []\n",
    "sent_lens = []\n",
    "num_read = 0\n",
    "\n",
    "for i in range(len(documents)):\n",
    "    # periodic heartbeat report\n",
    "    if num_read % 100 == 0:\n",
    "        print(\"building features from {:d} docs\".format(num_read))\n",
    "    # skip docs without specified topic\n",
    "    title_body = reuters.raw(documents[i]).lower()\n",
    "    if len(title_body) == 0:\n",
    "        continue\n",
    "    num_read += 1\n",
    "    # convert to list of word indexes\n",
    "    title_body = re.sub(\"\\n\", \"\", title_body)\n",
    "    for sent in nltk.sent_tokenize(title_body):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            if is_number(word):\n",
    "                word = \"9\"\n",
    "            word = word.lower()\n",
    "            word_freqs[word] += 1\n",
    "        sents.append(sent)\n",
    "        sent_lens.append(len(sent))\n",
    "        \n",
    "#ftext.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c64cbefc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sentences are: 50470 \n",
      "Sentence distribution min 1, max 3688 , mean 167.072657, median 155.000000\n",
      "Vocab size (full) 33743\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of sentences are: {:d} \".format(len(sents)))\n",
    "print (\"Sentence distribution min {:d}, max {:d} , mean {:3f}, median {:3f}\".format(np.min(sent_lens), np.max(sent_lens), np.mean(sent_lens), np.median(sent_lens)))\n",
    "print(\"Vocab size (full) {:d}\".format(len(word_freqs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f263c6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 5000\n",
    "EMBED_SIZE = 50\n",
    "LATENT_SIZE = 512\n",
    "SEQUENCE_LEN = 50\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2b558cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id = {}\n",
    "word2id[\"PAD\"] = 0\n",
    "word2id[\"UNK\"] = 1\n",
    "for v, (k, _) in enumerate(word_freqs.most_common(VOCAB_SIZE - 2)):\n",
    "    word2id[k] = v + 2\n",
    "id2word = {v:k for k, v in word2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec750e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_word2id(word):\n",
    "    try:\n",
    "        return word2id[word]\n",
    "    except KeyError:\n",
    "        return word2id[\"UNK\"]\n",
    "    \n",
    "def load_glove_vectors(glove_file, word2id, embed_size):\n",
    "    embedding = np.zeros((len(word2id), embed_size))\n",
    "    fglove = open(glove_file, \"rb\")\n",
    "    for line in fglove:\n",
    "        cols = line.strip().split()\n",
    "        word = cols[0].decode('utf-8')\n",
    "        if embed_size == 0:\n",
    "            embed_size = len(cols) - 1\n",
    "        if word in word2id:\n",
    "            vec = np.array([float(v) for v in cols[1:]])\n",
    "        embedding[lookup_word2id(word)] = vec\n",
    "    embedding[word2id[\"PAD\"]] = np.zeros((embed_size))\n",
    "    embedding[word2id[\"UNK\"]] = np.random.uniform(-1, 1, embed_size)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa3ed5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_wids = [[lookup_word2id(w) for w in s.split()] for s in sents]\n",
    "sent_wids = sequence.pad_sequences(sent_wids, SEQUENCE_LEN)\n",
    "# load glove vectors into weight matrix\n",
    "embeddings = load_glove_vectors(\"glove.6B.{:d}d.txt\".format(EMBED_SIZE), word2id, EMBED_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f39994d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_generator(X, embeddings, batch_size):\n",
    "    while True:\n",
    "        # loop once per epoch\n",
    "        num_recs = X.shape[0]\n",
    "        indices = np.random.permutation(np.arange(num_recs))\n",
    "        num_batches = num_recs // batch_size\n",
    "        for bid in range(num_batches):\n",
    "            sids = indices[bid * batch_size: (bid+1) * batch_size]\n",
    "            Xbatch = embeddings[X[sids, :]]\n",
    "            yield Xbatch, Xbatch\n",
    "            \n",
    "train_size = 0.7\n",
    "Xtrain, Xtest = train_test_split(sent_wids, train_size=train_size)\n",
    "train_gen = sentence_generator(Xtrain, embeddings, BATCH_SIZE)\n",
    "test_gen = sentence_generator(Xtest, embeddings, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbbe5793",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-31 17:52:56.661267: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape=(SEQUENCE_LEN, EMBED_SIZE), name=\"input\")\n",
    "encoded = Bidirectional(LSTM(LATENT_SIZE), merge_mode=\"sum\", name=\"encoder_lstm\")(inputs)\n",
    "decoded = RepeatVector(SEQUENCE_LEN, name=\"repeater\")(encoded)\n",
    "decoded = Bidirectional(LSTM(EMBED_SIZE, return_sequences=True), merge_mode=\"sum\", name=\"decoder_lstm\")(decoded)\n",
    "autoencoder = Model(inputs, decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4791ba8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer=\"adam\", loss=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbeec0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "552/552 [==============================] - 423s 740ms/step - loss: 0.1787 - val_loss: 0.1689\n",
      "Epoch 2/10\n",
      "552/552 [==============================] - 419s 759ms/step - loss: 0.1642 - val_loss: 0.1629\n",
      "Epoch 3/10\n",
      "552/552 [==============================] - 405s 733ms/step - loss: 0.1601 - val_loss: 0.1581\n",
      "Epoch 4/10\n",
      "552/552 [==============================] - 415s 753ms/step - loss: 0.1569 - val_loss: 0.1569\n",
      "Epoch 5/10\n",
      "196/552 [=========>....................] - ETA: 3:50 - loss: 0.1543"
     ]
    }
   ],
   "source": [
    "num_train_steps = len(Xtrain) // BATCH_SIZE\n",
    "num_test_steps = len(Xtest) // BATCH_SIZE\n",
    "steps_per_epoch=num_train_steps,\n",
    "epochs=NUM_EPOCHS,\n",
    "validation_data=test_gen,\n",
    "validation_steps=num_test_steps,\n",
    "history = autoencoder.fit(train_gen,\n",
    "                                    steps_per_epoch=num_train_steps,\n",
    "                                    epochs=NUM_EPOCHS,\n",
    "                                    validation_data=test_gen,\n",
    "                                    validation_steps=num_test_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8d09dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf29ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect autoencoder predictions for test set\n",
    "test_inputs, test_labels = next(test_gen)\n",
    "preds = autoencoder.predict(test_inputs)\n",
    "\n",
    "# extract encoder part from autoencoder\n",
    "encoder = Model(autoencoder.input,\n",
    "                autoencoder.get_layer(\"encoder_lstm\").output)\n",
    "# encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1e9e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"], label = \"training loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label = \"validation loss\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6270a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute difference between vector produced by original and autoencoded\n",
    "k = 500\n",
    "cosims = np.zeros((k))\n",
    "i = 0\n",
    "for bid in range(num_test_steps):\n",
    "    xtest, ytest = next(test_gen)\n",
    "    ytest_ = autoencoder.predict(xtest)\n",
    "    Xvec = encoder.predict(xtest)\n",
    "    Yvec = encoder.predict(ytest_)\n",
    "    for rid in range(Xvec.shape[0]):\n",
    "        if i >= k:\n",
    "            break\n",
    "        cosims[i] = compute_cosine_similarity(Xvec[rid], Yvec[rid])\n",
    "        if i <= 10:\n",
    "            print(cosims[i])\n",
    "        i += 1\n",
    "    if i >= k:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2783286",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(cosims, bins=10, density=True)\n",
    "plt.xlabel(\"cosine similarity\")\n",
    "plt.ylabel(\"frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6233b85e",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
