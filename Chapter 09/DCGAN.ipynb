{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ae945ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-01 18:00:51.623806: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-01 18:00:51.706091: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-01 18:00:51.707300: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-01 18:00:53.726782: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import UpSampling2D, Conv2D\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "422dc5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCGAN():\n",
    "    def __init__(self, rows, cols, channels, z = 10):\n",
    "        # Input shape\n",
    "        self.img_rows = rows\n",
    "        self.img_cols = cols\n",
    "        self.channels = channels\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = z\n",
    "\n",
    "        optimizer = tf.keras.optimizers.legacy.Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise as input and generates imgs\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        valid = self.discriminator(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(z, valid)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(128 * 7 * 7, activation=\"relu\", input_dim=self.latent_dim))\n",
    "        model.add(Reshape((7, 7, 128)))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Conv2D(self.channels, kernel_size=3, padding=\"same\"))\n",
    "        model.add(Activation(\"tanh\"))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(self, epochs, batch_size=256, save_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        (X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = X_train / 127.5 - 1.\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random half of images\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            imgs = X_train[idx]\n",
    "\n",
    "            # Sample noise and generate a batch of new images\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "            gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "            # Train the discriminator (real classified as ones and generated as zeros)\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            # Train the generator (wants discriminator to mistake images as real)\n",
    "            g_loss = self.combined.train_on_batch(noise, valid)\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % save_interval == 0:\n",
    "                self.save_imgs(epoch)\n",
    "\n",
    "    def save_imgs(self, epoch):\n",
    "        r, c = 5, 5\n",
    "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(\"images/dcgan_mnist_%d.png\" % epoch)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df97a377",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-01 18:00:56.413765: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 14, 14, 32)        320       \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 14, 14, 32)        0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 14, 14, 32)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 7, 7, 64)          18496     \n",
      "                                                                 \n",
      " zero_padding2d (ZeroPaddin  (None, 8, 8, 64)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 8, 8, 64)          256       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 8, 8, 64)          0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 8, 8, 64)          0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 4, 4, 128)         73856     \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 4, 4, 128)         512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 4, 4, 256)         295168    \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 4, 4, 256)         1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 4096)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 4097      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 393729 (1.50 MB)\n",
      "Trainable params: 392833 (1.50 MB)\n",
      "Non-trainable params: 896 (3.50 KB)\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1 (Dense)             (None, 6272)              68992     \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 7, 7, 128)         0         \n",
      "                                                                 \n",
      " up_sampling2d (UpSampling2  (None, 14, 14, 128)       0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 14, 14, 128)       147584    \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 14, 14, 128)       512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation (Activation)     (None, 14, 14, 128)       0         \n",
      "                                                                 \n",
      " up_sampling2d_1 (UpSamplin  (None, 28, 28, 128)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 28, 28, 64)        73792     \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 28, 28, 64)        256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 28, 28, 64)        0         \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 28, 28, 1)         577       \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 28, 28, 1)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 291713 (1.11 MB)\n",
      "Trainable params: 291329 (1.11 MB)\n",
      "Non-trainable params: 384 (1.50 KB)\n",
      "_________________________________________________________________\n",
      "8/8 [==============================] - 1s 52ms/step\n",
      "0 [D loss: 1.264250, acc.: 44.53%] [G loss: 0.658831]\n",
      "1/1 [==============================] - 0s 241ms/step\n",
      "8/8 [==============================] - 0s 51ms/step\n",
      "1 [D loss: 0.588159, acc.: 64.84%] [G loss: 0.678166]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "2 [D loss: 0.142927, acc.: 98.83%] [G loss: 0.693664]\n",
      "8/8 [==============================] - 0s 51ms/step\n",
      "3 [D loss: 0.084170, acc.: 99.41%] [G loss: 0.608886]\n",
      "8/8 [==============================] - 0s 51ms/step\n",
      "4 [D loss: 0.045765, acc.: 100.00%] [G loss: 0.441823]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "5 [D loss: 0.053844, acc.: 100.00%] [G loss: 0.250910]\n",
      "8/8 [==============================] - 0s 50ms/step\n",
      "6 [D loss: 0.545843, acc.: 68.75%] [G loss: 0.282819]\n",
      "8/8 [==============================] - 0s 50ms/step\n",
      "7 [D loss: 0.592724, acc.: 63.67%] [G loss: 0.716789]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "8 [D loss: 0.305164, acc.: 89.65%] [G loss: 0.468682]\n",
      "8/8 [==============================] - 0s 50ms/step\n",
      "9 [D loss: 0.237439, acc.: 93.16%] [G loss: 0.100076]\n",
      "8/8 [==============================] - 0s 54ms/step\n",
      "10 [D loss: 0.157230, acc.: 96.68%] [G loss: 0.035952]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "11 [D loss: 0.160708, acc.: 96.09%] [G loss: 0.033490]\n",
      "8/8 [==============================] - 0s 49ms/step\n",
      "12 [D loss: 0.230762, acc.: 92.77%] [G loss: 0.053781]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "13 [D loss: 0.259007, acc.: 91.21%] [G loss: 0.083049]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "14 [D loss: 0.302996, acc.: 90.82%] [G loss: 0.112054]\n",
      "8/8 [==============================] - 0s 49ms/step\n",
      "15 [D loss: 0.410183, acc.: 82.81%] [G loss: 0.157152]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "16 [D loss: 0.260627, acc.: 91.80%] [G loss: 0.228583]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "17 [D loss: 0.357599, acc.: 85.16%] [G loss: 0.326226]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "18 [D loss: 0.483568, acc.: 76.37%] [G loss: 0.834982]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "19 [D loss: 0.569233, acc.: 69.53%] [G loss: 1.227348]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "20 [D loss: 0.884541, acc.: 53.52%] [G loss: 2.251758]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "21 [D loss: 0.425999, acc.: 78.32%] [G loss: 1.064905]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "22 [D loss: 0.405113, acc.: 81.64%] [G loss: 0.973118]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "23 [D loss: 0.354691, acc.: 84.77%] [G loss: 1.585442]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "24 [D loss: 0.810416, acc.: 55.27%] [G loss: 2.410504]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "25 [D loss: 0.792774, acc.: 59.38%] [G loss: 0.895025]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 47ms/step\n",
      "26 [D loss: 0.631039, acc.: 67.19%] [G loss: 0.796637]\n",
      "8/8 [==============================] - 0s 51ms/step\n",
      "27 [D loss: 0.414148, acc.: 80.86%] [G loss: 0.757792]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "28 [D loss: 0.319247, acc.: 87.89%] [G loss: 0.514823]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "29 [D loss: 0.217928, acc.: 93.55%] [G loss: 0.477493]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "30 [D loss: 0.263641, acc.: 88.67%] [G loss: 0.565768]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "31 [D loss: 0.702416, acc.: 64.06%] [G loss: 1.637596]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "32 [D loss: 0.851546, acc.: 52.15%] [G loss: 1.956462]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "33 [D loss: 1.035523, acc.: 41.41%] [G loss: 1.701118]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "34 [D loss: 0.694549, acc.: 63.87%] [G loss: 1.241961]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "35 [D loss: 0.534829, acc.: 74.02%] [G loss: 1.029472]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "36 [D loss: 0.685371, acc.: 63.87%] [G loss: 1.102475]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "37 [D loss: 0.869354, acc.: 51.37%] [G loss: 1.915508]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "38 [D loss: 1.347189, acc.: 30.08%] [G loss: 2.126117]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "39 [D loss: 1.548726, acc.: 20.90%] [G loss: 1.774237]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "40 [D loss: 1.265191, acc.: 34.96%] [G loss: 1.173418]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "41 [D loss: 1.229654, acc.: 32.62%] [G loss: 1.079868]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "42 [D loss: 1.053922, acc.: 40.43%] [G loss: 1.081475]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "43 [D loss: 0.791206, acc.: 56.25%] [G loss: 0.906569]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "44 [D loss: 0.541516, acc.: 73.83%] [G loss: 0.942716]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "45 [D loss: 0.346611, acc.: 86.33%] [G loss: 1.154212]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "46 [D loss: 0.468313, acc.: 75.59%] [G loss: 1.379222]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "47 [D loss: 0.726487, acc.: 59.18%] [G loss: 1.970698]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "48 [D loss: 1.280954, acc.: 31.25%] [G loss: 1.918935]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "49 [D loss: 1.480324, acc.: 23.44%] [G loss: 1.299840]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "50 [D loss: 1.123296, acc.: 37.30%] [G loss: 0.989417]\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "51 [D loss: 1.115634, acc.: 38.67%] [G loss: 1.039509]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "52 [D loss: 0.991290, acc.: 42.38%] [G loss: 1.076223]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "53 [D loss: 0.842079, acc.: 55.47%] [G loss: 1.018568]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "54 [D loss: 0.705974, acc.: 61.13%] [G loss: 1.068744]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "55 [D loss: 0.631625, acc.: 67.97%] [G loss: 1.073549]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "56 [D loss: 0.612148, acc.: 67.77%] [G loss: 1.029118]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "57 [D loss: 0.561951, acc.: 70.31%] [G loss: 0.962892]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "58 [D loss: 0.641242, acc.: 65.62%] [G loss: 1.151847]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "59 [D loss: 0.723729, acc.: 60.74%] [G loss: 1.231262]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "60 [D loss: 0.908274, acc.: 48.63%] [G loss: 1.460689]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "61 [D loss: 1.251165, acc.: 28.12%] [G loss: 1.401940]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "62 [D loss: 1.350350, acc.: 24.80%] [G loss: 1.271202]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "63 [D loss: 1.298504, acc.: 25.00%] [G loss: 1.083810]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "64 [D loss: 1.127387, acc.: 33.59%] [G loss: 1.129613]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "65 [D loss: 1.057391, acc.: 38.28%] [G loss: 1.008497]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "66 [D loss: 0.916490, acc.: 45.51%] [G loss: 1.285576]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "67 [D loss: 0.885542, acc.: 48.44%] [G loss: 1.205843]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "68 [D loss: 0.861835, acc.: 49.22%] [G loss: 1.338112]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "69 [D loss: 0.808586, acc.: 51.37%] [G loss: 1.242031]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "70 [D loss: 0.801960, acc.: 53.91%] [G loss: 1.288233]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "71 [D loss: 0.732822, acc.: 57.23%] [G loss: 1.255301]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "72 [D loss: 0.860875, acc.: 48.63%] [G loss: 1.160640]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "73 [D loss: 0.952030, acc.: 46.68%] [G loss: 1.244226]\n",
      "8/8 [==============================] - 0s 56ms/step\n",
      "74 [D loss: 0.898618, acc.: 47.46%] [G loss: 1.224304]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "75 [D loss: 0.962111, acc.: 41.02%] [G loss: 1.160989]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "76 [D loss: 0.979829, acc.: 41.99%] [G loss: 1.242730]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "77 [D loss: 1.044905, acc.: 38.87%] [G loss: 1.230380]\n",
      "8/8 [==============================] - 0s 50ms/step\n",
      "78 [D loss: 0.915759, acc.: 45.12%] [G loss: 1.154826]\n",
      "8/8 [==============================] - 0s 49ms/step\n",
      "79 [D loss: 0.846609, acc.: 48.63%] [G loss: 1.093864]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "80 [D loss: 0.909430, acc.: 47.66%] [G loss: 1.197108]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "81 [D loss: 0.884469, acc.: 48.63%] [G loss: 1.178665]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "82 [D loss: 0.774987, acc.: 55.86%] [G loss: 1.255079]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "83 [D loss: 0.824181, acc.: 50.98%] [G loss: 1.199865]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "84 [D loss: 0.714609, acc.: 59.18%] [G loss: 1.373744]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "85 [D loss: 0.797273, acc.: 53.52%] [G loss: 1.304550]\n",
      "8/8 [==============================] - 0s 54ms/step\n",
      "86 [D loss: 0.832205, acc.: 49.41%] [G loss: 1.360204]\n",
      "8/8 [==============================] - 0s 53ms/step\n",
      "87 [D loss: 0.989989, acc.: 38.28%] [G loss: 1.301016]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "88 [D loss: 0.910581, acc.: 43.95%] [G loss: 1.346339]\n",
      "8/8 [==============================] - 0s 50ms/step\n",
      "89 [D loss: 0.937733, acc.: 44.34%] [G loss: 1.240980]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "90 [D loss: 0.960233, acc.: 43.16%] [G loss: 1.125676]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "91 [D loss: 0.959505, acc.: 44.34%] [G loss: 1.155497]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "92 [D loss: 0.857114, acc.: 49.61%] [G loss: 1.150671]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "93 [D loss: 0.811168, acc.: 50.39%] [G loss: 1.142647]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "94 [D loss: 0.861395, acc.: 48.83%] [G loss: 1.140891]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "95 [D loss: 0.826701, acc.: 50.20%] [G loss: 1.144917]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "96 [D loss: 0.775984, acc.: 57.62%] [G loss: 1.184510]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "97 [D loss: 0.808945, acc.: 56.05%] [G loss: 1.185779]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "98 [D loss: 0.810975, acc.: 54.30%] [G loss: 1.057796]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "99 [D loss: 0.818135, acc.: 47.66%] [G loss: 1.209021]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "100 [D loss: 0.844298, acc.: 48.83%] [G loss: 1.195993]\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "101 [D loss: 0.850193, acc.: 49.41%] [G loss: 1.130676]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 49ms/step\n",
      "102 [D loss: 0.759228, acc.: 58.40%] [G loss: 1.188369]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "103 [D loss: 0.812224, acc.: 51.76%] [G loss: 1.213637]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "104 [D loss: 0.798999, acc.: 51.95%] [G loss: 1.225005]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "105 [D loss: 0.711207, acc.: 60.35%] [G loss: 1.140019]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "106 [D loss: 0.743033, acc.: 57.23%] [G loss: 1.142313]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "107 [D loss: 0.711528, acc.: 55.86%] [G loss: 1.135073]\n",
      "8/8 [==============================] - 0s 52ms/step\n",
      "108 [D loss: 0.780103, acc.: 52.34%] [G loss: 1.109139]\n",
      "8/8 [==============================] - 0s 49ms/step\n",
      "109 [D loss: 0.808703, acc.: 51.95%] [G loss: 1.037673]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "110 [D loss: 0.847453, acc.: 47.27%] [G loss: 1.012392]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "111 [D loss: 0.845852, acc.: 49.02%] [G loss: 1.067168]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "112 [D loss: 0.846757, acc.: 50.20%] [G loss: 1.058564]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "113 [D loss: 0.965708, acc.: 40.43%] [G loss: 1.179262]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "114 [D loss: 0.861926, acc.: 46.88%] [G loss: 1.145931]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "115 [D loss: 0.795712, acc.: 52.34%] [G loss: 1.261646]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "116 [D loss: 0.756783, acc.: 56.05%] [G loss: 1.211266]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "117 [D loss: 0.818340, acc.: 49.41%] [G loss: 1.132066]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "118 [D loss: 0.819406, acc.: 51.37%] [G loss: 1.178097]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "119 [D loss: 0.832661, acc.: 45.90%] [G loss: 1.109704]\n",
      "8/8 [==============================] - 0s 50ms/step\n",
      "120 [D loss: 0.787506, acc.: 52.15%] [G loss: 1.210671]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "121 [D loss: 0.832910, acc.: 49.41%] [G loss: 1.107565]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "122 [D loss: 0.849762, acc.: 48.83%] [G loss: 1.093371]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "123 [D loss: 0.792105, acc.: 52.93%] [G loss: 1.138900]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "124 [D loss: 0.812593, acc.: 50.00%] [G loss: 1.063856]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "125 [D loss: 0.761786, acc.: 56.25%] [G loss: 1.137104]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "126 [D loss: 0.833425, acc.: 48.44%] [G loss: 1.121104]\n",
      "8/8 [==============================] - 0s 52ms/step\n",
      "127 [D loss: 0.722001, acc.: 56.84%] [G loss: 1.100057]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "128 [D loss: 0.719957, acc.: 57.81%] [G loss: 1.165851]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "129 [D loss: 0.762091, acc.: 53.71%] [G loss: 1.208132]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "130 [D loss: 0.710487, acc.: 58.59%] [G loss: 1.146165]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "131 [D loss: 0.651351, acc.: 60.55%] [G loss: 1.154325]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "132 [D loss: 0.682365, acc.: 60.74%] [G loss: 1.058040]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "133 [D loss: 0.713930, acc.: 57.81%] [G loss: 1.088089]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "134 [D loss: 0.811210, acc.: 49.22%] [G loss: 1.027530]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "135 [D loss: 0.881506, acc.: 46.68%] [G loss: 1.038080]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "136 [D loss: 0.796206, acc.: 50.59%] [G loss: 1.021636]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "137 [D loss: 0.911153, acc.: 38.87%] [G loss: 1.079735]\n",
      "8/8 [==============================] - 0s 52ms/step\n",
      "138 [D loss: 0.855126, acc.: 48.63%] [G loss: 1.051118]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "139 [D loss: 0.713851, acc.: 58.79%] [G loss: 1.246966]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "140 [D loss: 0.692525, acc.: 59.38%] [G loss: 1.163168]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "141 [D loss: 0.694316, acc.: 59.96%] [G loss: 1.159287]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "142 [D loss: 0.795764, acc.: 52.15%] [G loss: 1.258318]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "143 [D loss: 0.729912, acc.: 58.59%] [G loss: 1.181679]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "144 [D loss: 0.823952, acc.: 48.63%] [G loss: 1.195725]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "145 [D loss: 0.791104, acc.: 52.34%] [G loss: 1.168883]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "146 [D loss: 0.802390, acc.: 53.12%] [G loss: 1.238684]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "147 [D loss: 0.818606, acc.: 48.05%] [G loss: 1.225201]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "148 [D loss: 0.835779, acc.: 47.66%] [G loss: 1.302613]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "149 [D loss: 0.883577, acc.: 43.95%] [G loss: 1.188055]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "150 [D loss: 0.802035, acc.: 52.15%] [G loss: 1.233423]\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "151 [D loss: 0.748984, acc.: 52.54%] [G loss: 1.177615]\n",
      "8/8 [==============================] - 0s 49ms/step\n",
      "152 [D loss: 0.788837, acc.: 54.69%] [G loss: 1.164951]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "153 [D loss: 0.795801, acc.: 50.59%] [G loss: 1.222109]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "154 [D loss: 0.791591, acc.: 48.83%] [G loss: 1.095348]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "155 [D loss: 0.871652, acc.: 43.55%] [G loss: 1.157185]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "156 [D loss: 0.805766, acc.: 51.56%] [G loss: 1.170309]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "157 [D loss: 0.847344, acc.: 45.90%] [G loss: 1.039390]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "158 [D loss: 0.760412, acc.: 52.15%] [G loss: 1.140431]\n",
      "8/8 [==============================] - 0s 49ms/step\n",
      "159 [D loss: 0.755396, acc.: 52.73%] [G loss: 1.147475]\n",
      "8/8 [==============================] - 0s 49ms/step\n",
      "160 [D loss: 0.747586, acc.: 56.45%] [G loss: 1.077494]\n",
      "8/8 [==============================] - 0s 49ms/step\n",
      "161 [D loss: 0.772689, acc.: 54.10%] [G loss: 1.023514]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "162 [D loss: 0.808992, acc.: 49.41%] [G loss: 1.106048]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "163 [D loss: 0.797268, acc.: 49.61%] [G loss: 1.049829]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "164 [D loss: 0.706567, acc.: 60.16%] [G loss: 1.072936]\n",
      "8/8 [==============================] - 0s 52ms/step\n",
      "165 [D loss: 0.698502, acc.: 57.42%] [G loss: 1.020030]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "166 [D loss: 0.753474, acc.: 55.27%] [G loss: 1.043780]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "167 [D loss: 0.768050, acc.: 55.27%] [G loss: 1.119879]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "168 [D loss: 0.698024, acc.: 59.57%] [G loss: 1.154959]\n",
      "8/8 [==============================] - 0s 49ms/step\n",
      "169 [D loss: 0.806577, acc.: 52.15%] [G loss: 1.194322]\n",
      "8/8 [==============================] - 0s 49ms/step\n",
      "170 [D loss: 0.738611, acc.: 53.52%] [G loss: 1.138835]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "171 [D loss: 0.751609, acc.: 52.15%] [G loss: 1.144564]\n",
      "8/8 [==============================] - 0s 49ms/step\n",
      "172 [D loss: 0.715383, acc.: 57.81%] [G loss: 1.200421]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "173 [D loss: 0.855934, acc.: 46.09%] [G loss: 1.067461]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "174 [D loss: 0.852348, acc.: 45.90%] [G loss: 1.138937]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "175 [D loss: 0.875149, acc.: 46.88%] [G loss: 1.110056]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "176 [D loss: 0.852755, acc.: 47.85%] [G loss: 1.104433]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 49ms/step\n",
      "177 [D loss: 0.737845, acc.: 56.05%] [G loss: 1.046140]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "178 [D loss: 0.777501, acc.: 52.34%] [G loss: 0.989368]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "179 [D loss: 0.756753, acc.: 55.86%] [G loss: 0.999487]\n",
      "8/8 [==============================] - 0s 49ms/step\n",
      "180 [D loss: 0.703269, acc.: 59.38%] [G loss: 1.008132]\n",
      "8/8 [==============================] - 0s 51ms/step\n",
      "181 [D loss: 0.734251, acc.: 55.27%] [G loss: 1.080739]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "182 [D loss: 0.680253, acc.: 61.52%] [G loss: 1.115381]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "183 [D loss: 0.833149, acc.: 47.66%] [G loss: 1.088480]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "184 [D loss: 0.781517, acc.: 51.76%] [G loss: 1.173433]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "185 [D loss: 0.858239, acc.: 43.36%] [G loss: 1.145571]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "186 [D loss: 0.835563, acc.: 49.61%] [G loss: 1.191088]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "187 [D loss: 0.846080, acc.: 49.61%] [G loss: 1.150574]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "188 [D loss: 0.784735, acc.: 51.37%] [G loss: 1.020269]\n",
      "8/8 [==============================] - 0s 49ms/step\n",
      "189 [D loss: 0.797499, acc.: 48.24%] [G loss: 1.028972]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "190 [D loss: 0.794468, acc.: 51.76%] [G loss: 1.115844]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "191 [D loss: 0.760875, acc.: 55.27%] [G loss: 1.044396]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "192 [D loss: 0.766981, acc.: 55.27%] [G loss: 1.201386]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "193 [D loss: 0.782655, acc.: 53.12%] [G loss: 1.092098]\n",
      "8/8 [==============================] - 0s 49ms/step\n",
      "194 [D loss: 0.758746, acc.: 54.30%] [G loss: 1.047845]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "195 [D loss: 0.695769, acc.: 56.84%] [G loss: 1.086617]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "196 [D loss: 0.768947, acc.: 53.71%] [G loss: 0.955316]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "197 [D loss: 0.789744, acc.: 53.91%] [G loss: 1.023786]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "198 [D loss: 0.825994, acc.: 49.80%] [G loss: 1.056410]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "199 [D loss: 0.841399, acc.: 47.46%] [G loss: 1.045823]\n",
      "8/8 [==============================] - 0s 53ms/step\n",
      "200 [D loss: 0.779566, acc.: 50.78%] [G loss: 1.064550]\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "201 [D loss: 0.716885, acc.: 54.69%] [G loss: 1.128037]\n",
      "8/8 [==============================] - 0s 53ms/step\n",
      "202 [D loss: 0.759385, acc.: 55.08%] [G loss: 1.180459]\n",
      "8/8 [==============================] - 0s 50ms/step\n",
      "203 [D loss: 0.726570, acc.: 57.62%] [G loss: 1.199287]\n",
      "8/8 [==============================] - 0s 54ms/step\n",
      "204 [D loss: 0.724464, acc.: 57.62%] [G loss: 1.142835]\n",
      "8/8 [==============================] - 0s 49ms/step\n",
      "205 [D loss: 0.704579, acc.: 58.01%] [G loss: 1.151075]\n",
      "8/8 [==============================] - 0s 54ms/step\n",
      "206 [D loss: 0.653711, acc.: 61.72%] [G loss: 1.125286]\n",
      "8/8 [==============================] - 0s 50ms/step\n",
      "207 [D loss: 0.690915, acc.: 60.74%] [G loss: 0.961818]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "208 [D loss: 0.710117, acc.: 58.79%] [G loss: 0.919480]\n",
      "8/8 [==============================] - 1s 61ms/step\n",
      "209 [D loss: 0.714893, acc.: 56.84%] [G loss: 0.903477]\n",
      "8/8 [==============================] - 0s 52ms/step\n",
      "210 [D loss: 0.860302, acc.: 46.29%] [G loss: 0.923200]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "211 [D loss: 0.850796, acc.: 45.31%] [G loss: 0.938767]\n",
      "8/8 [==============================] - 0s 49ms/step\n",
      "212 [D loss: 0.745742, acc.: 55.08%] [G loss: 1.060585]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "213 [D loss: 0.727006, acc.: 58.98%] [G loss: 1.064575]\n",
      "8/8 [==============================] - 0s 59ms/step\n",
      "214 [D loss: 0.656690, acc.: 63.87%] [G loss: 1.152733]\n",
      "8/8 [==============================] - 0s 53ms/step\n",
      "215 [D loss: 0.703337, acc.: 59.77%] [G loss: 1.137994]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "216 [D loss: 0.671607, acc.: 59.96%] [G loss: 1.189739]\n",
      "8/8 [==============================] - 0s 55ms/step\n",
      "217 [D loss: 0.780941, acc.: 53.12%] [G loss: 1.155642]\n",
      "8/8 [==============================] - 0s 53ms/step\n",
      "218 [D loss: 0.732403, acc.: 57.23%] [G loss: 1.289824]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "219 [D loss: 0.787620, acc.: 52.15%] [G loss: 1.271617]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "220 [D loss: 0.802489, acc.: 49.22%] [G loss: 1.179343]\n",
      "8/8 [==============================] - 0s 53ms/step\n",
      "221 [D loss: 0.831106, acc.: 48.44%] [G loss: 1.169720]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "222 [D loss: 0.794002, acc.: 49.41%] [G loss: 1.181837]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "223 [D loss: 0.720097, acc.: 56.84%] [G loss: 1.110013]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "224 [D loss: 0.749784, acc.: 54.69%] [G loss: 1.095370]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "225 [D loss: 0.649242, acc.: 63.67%] [G loss: 1.052766]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "226 [D loss: 0.679297, acc.: 60.35%] [G loss: 0.979002]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "227 [D loss: 0.662777, acc.: 61.33%] [G loss: 1.001990]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "228 [D loss: 0.726579, acc.: 54.49%] [G loss: 0.973007]\n",
      "8/8 [==============================] - 0s 49ms/step\n",
      "229 [D loss: 0.733611, acc.: 57.23%] [G loss: 1.094219]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "230 [D loss: 0.772150, acc.: 52.54%] [G loss: 1.055450]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "231 [D loss: 0.781254, acc.: 53.91%] [G loss: 1.134467]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "232 [D loss: 0.824223, acc.: 43.95%] [G loss: 1.232494]\n",
      "8/8 [==============================] - 0s 50ms/step\n",
      "233 [D loss: 0.819185, acc.: 46.88%] [G loss: 1.191503]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "234 [D loss: 0.767851, acc.: 53.12%] [G loss: 1.244958]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "235 [D loss: 0.741471, acc.: 54.30%] [G loss: 1.237429]\n",
      "8/8 [==============================] - 0s 51ms/step\n",
      "236 [D loss: 0.804565, acc.: 48.44%] [G loss: 1.205731]\n",
      "8/8 [==============================] - 0s 49ms/step\n",
      "237 [D loss: 0.834033, acc.: 46.48%] [G loss: 1.138682]\n",
      "8/8 [==============================] - 0s 53ms/step\n",
      "238 [D loss: 0.803763, acc.: 48.24%] [G loss: 1.151123]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "239 [D loss: 0.909316, acc.: 40.04%] [G loss: 1.073882]\n",
      "8/8 [==============================] - 0s 49ms/step\n",
      "240 [D loss: 0.854484, acc.: 44.34%] [G loss: 1.140594]\n",
      "8/8 [==============================] - 0s 50ms/step\n",
      "241 [D loss: 0.875894, acc.: 43.36%] [G loss: 1.027758]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "242 [D loss: 0.824629, acc.: 47.66%] [G loss: 1.062472]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "243 [D loss: 0.722392, acc.: 56.25%] [G loss: 1.109208]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "244 [D loss: 0.791609, acc.: 52.15%] [G loss: 1.057240]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "245 [D loss: 0.721042, acc.: 56.05%] [G loss: 1.177012]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "246 [D loss: 0.734066, acc.: 56.84%] [G loss: 1.063574]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "247 [D loss: 0.714853, acc.: 57.81%] [G loss: 1.173869]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "248 [D loss: 0.700489, acc.: 56.45%] [G loss: 1.203091]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "249 [D loss: 0.833077, acc.: 46.68%] [G loss: 1.151214]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "250 [D loss: 0.716614, acc.: 55.47%] [G loss: 1.134493]\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "251 [D loss: 0.725353, acc.: 56.25%] [G loss: 1.127628]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 48ms/step\n",
      "252 [D loss: 0.833606, acc.: 47.85%] [G loss: 1.046008]\n",
      "8/8 [==============================] - 0s 50ms/step\n",
      "253 [D loss: 0.865115, acc.: 42.58%] [G loss: 1.011741]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "254 [D loss: 0.830962, acc.: 48.83%] [G loss: 1.141884]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "255 [D loss: 0.809414, acc.: 47.66%] [G loss: 1.116294]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "256 [D loss: 0.786315, acc.: 51.56%] [G loss: 1.055007]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "257 [D loss: 0.679313, acc.: 60.35%] [G loss: 1.057359]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "258 [D loss: 0.636670, acc.: 63.09%] [G loss: 1.040301]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "259 [D loss: 0.691901, acc.: 57.42%] [G loss: 0.958848]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "260 [D loss: 0.627708, acc.: 64.65%] [G loss: 1.042243]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "261 [D loss: 0.670628, acc.: 60.16%] [G loss: 1.006880]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "262 [D loss: 0.674189, acc.: 59.77%] [G loss: 1.044410]\n",
      "8/8 [==============================] - 0s 49ms/step\n",
      "263 [D loss: 0.792957, acc.: 52.15%] [G loss: 1.032997]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "264 [D loss: 0.723201, acc.: 58.20%] [G loss: 1.056284]\n",
      "8/8 [==============================] - 0s 49ms/step\n",
      "265 [D loss: 0.763615, acc.: 48.63%] [G loss: 1.050675]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "266 [D loss: 0.834142, acc.: 46.88%] [G loss: 1.064357]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "267 [D loss: 0.835608, acc.: 44.92%] [G loss: 1.101878]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "268 [D loss: 0.773212, acc.: 51.76%] [G loss: 1.069905]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "269 [D loss: 0.807067, acc.: 47.85%] [G loss: 1.130085]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "270 [D loss: 0.766410, acc.: 50.78%] [G loss: 1.227507]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "271 [D loss: 0.774682, acc.: 51.76%] [G loss: 1.164728]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "272 [D loss: 0.710323, acc.: 58.59%] [G loss: 1.234486]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "273 [D loss: 0.797266, acc.: 47.46%] [G loss: 1.224939]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "274 [D loss: 0.760670, acc.: 50.78%] [G loss: 1.101597]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "275 [D loss: 0.733602, acc.: 55.27%] [G loss: 1.135230]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "276 [D loss: 0.721114, acc.: 54.49%] [G loss: 1.094667]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "277 [D loss: 0.794076, acc.: 50.78%] [G loss: 1.012781]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "278 [D loss: 0.733644, acc.: 55.08%] [G loss: 0.993560]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "279 [D loss: 0.790513, acc.: 53.12%] [G loss: 0.993469]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "280 [D loss: 0.766264, acc.: 51.76%] [G loss: 0.947515]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "281 [D loss: 0.732490, acc.: 53.91%] [G loss: 0.993679]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "282 [D loss: 0.768968, acc.: 51.17%] [G loss: 0.926187]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "283 [D loss: 0.708229, acc.: 59.18%] [G loss: 1.018348]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "284 [D loss: 0.655836, acc.: 62.89%] [G loss: 0.916019]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "285 [D loss: 0.635159, acc.: 62.30%] [G loss: 0.990662]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "286 [D loss: 0.673964, acc.: 59.96%] [G loss: 1.073812]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "287 [D loss: 0.747739, acc.: 53.52%] [G loss: 1.063009]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "288 [D loss: 0.715053, acc.: 56.45%] [G loss: 1.100514]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "289 [D loss: 0.735485, acc.: 56.25%] [G loss: 1.108176]\n",
      "8/8 [==============================] - 0s 49ms/step\n",
      "290 [D loss: 0.784537, acc.: 49.80%] [G loss: 1.111567]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "291 [D loss: 0.836443, acc.: 45.51%] [G loss: 1.141723]\n",
      "8/8 [==============================] - 0s 53ms/step\n",
      "292 [D loss: 0.797015, acc.: 49.61%] [G loss: 1.013266]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "293 [D loss: 0.861753, acc.: 45.51%] [G loss: 0.988571]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "294 [D loss: 0.808917, acc.: 48.83%] [G loss: 0.983882]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "295 [D loss: 0.847175, acc.: 46.09%] [G loss: 0.925987]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "296 [D loss: 0.782048, acc.: 49.22%] [G loss: 0.974316]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "297 [D loss: 0.745569, acc.: 53.71%] [G loss: 0.904693]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "298 [D loss: 0.766967, acc.: 51.37%] [G loss: 1.020917]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "299 [D loss: 0.691824, acc.: 61.13%] [G loss: 1.067720]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "300 [D loss: 0.707734, acc.: 57.62%] [G loss: 1.061961]\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "301 [D loss: 0.698408, acc.: 58.01%] [G loss: 1.024792]\n",
      "8/8 [==============================] - 0s 51ms/step\n",
      "302 [D loss: 0.703890, acc.: 55.66%] [G loss: 1.101538]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "303 [D loss: 0.695546, acc.: 58.79%] [G loss: 1.084583]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "304 [D loss: 0.707856, acc.: 57.81%] [G loss: 1.107755]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "305 [D loss: 0.651234, acc.: 62.11%] [G loss: 1.078993]\n",
      "8/8 [==============================] - 0s 49ms/step\n",
      "306 [D loss: 0.737228, acc.: 54.69%] [G loss: 1.076069]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "307 [D loss: 0.687052, acc.: 61.33%] [G loss: 1.093483]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "308 [D loss: 0.736689, acc.: 54.49%] [G loss: 0.991531]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "309 [D loss: 0.782577, acc.: 49.22%] [G loss: 1.029174]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "310 [D loss: 0.787739, acc.: 48.83%] [G loss: 1.010413]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "311 [D loss: 0.767019, acc.: 50.59%] [G loss: 1.026739]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "312 [D loss: 0.749667, acc.: 52.73%] [G loss: 0.972021]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "313 [D loss: 0.740157, acc.: 53.91%] [G loss: 0.974831]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "314 [D loss: 0.762627, acc.: 53.71%] [G loss: 0.927248]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "315 [D loss: 0.732335, acc.: 53.32%] [G loss: 0.994240]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "316 [D loss: 0.673836, acc.: 61.72%] [G loss: 0.989276]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "317 [D loss: 0.642572, acc.: 64.26%] [G loss: 0.981185]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "318 [D loss: 0.597924, acc.: 68.95%] [G loss: 0.990817]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "319 [D loss: 0.633105, acc.: 63.67%] [G loss: 1.015360]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "320 [D loss: 0.620149, acc.: 66.02%] [G loss: 0.907625]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "321 [D loss: 0.532227, acc.: 75.59%] [G loss: 1.039084]\n",
      "8/8 [==============================] - 0s 49ms/step\n",
      "322 [D loss: 0.610168, acc.: 66.21%] [G loss: 0.997567]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "323 [D loss: 0.640674, acc.: 61.91%] [G loss: 1.037781]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "324 [D loss: 0.703505, acc.: 57.81%] [G loss: 1.048371]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "325 [D loss: 0.742146, acc.: 54.30%] [G loss: 1.097730]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "326 [D loss: 0.808357, acc.: 46.29%] [G loss: 1.093264]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 46ms/step\n",
      "327 [D loss: 0.899799, acc.: 37.89%] [G loss: 1.121085]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "328 [D loss: 0.839608, acc.: 47.85%] [G loss: 1.239891]\n",
      "8/8 [==============================] - 0s 49ms/step\n",
      "329 [D loss: 0.826639, acc.: 46.68%] [G loss: 1.157218]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "330 [D loss: 0.864784, acc.: 43.16%] [G loss: 1.031208]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "331 [D loss: 0.825589, acc.: 44.92%] [G loss: 1.083914]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "332 [D loss: 0.827485, acc.: 45.12%] [G loss: 1.005617]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "333 [D loss: 0.779957, acc.: 49.61%] [G loss: 0.921941]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "334 [D loss: 0.692845, acc.: 57.03%] [G loss: 0.929209]\n",
      "8/8 [==============================] - 0s 53ms/step\n",
      "335 [D loss: 0.670054, acc.: 61.52%] [G loss: 0.898425]\n",
      "8/8 [==============================] - 0s 54ms/step\n",
      "336 [D loss: 0.639511, acc.: 63.09%] [G loss: 0.965590]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "337 [D loss: 0.578935, acc.: 68.55%] [G loss: 0.958263]\n",
      "8/8 [==============================] - 0s 54ms/step\n",
      "338 [D loss: 0.645262, acc.: 62.30%] [G loss: 0.977307]\n",
      "8/8 [==============================] - 0s 57ms/step\n",
      "339 [D loss: 0.689266, acc.: 55.66%] [G loss: 1.021504]\n",
      "8/8 [==============================] - 0s 51ms/step\n",
      "340 [D loss: 0.689789, acc.: 58.79%] [G loss: 1.094198]\n",
      "8/8 [==============================] - 0s 59ms/step\n",
      "341 [D loss: 0.839862, acc.: 43.75%] [G loss: 1.105425]\n",
      "8/8 [==============================] - 0s 51ms/step\n",
      "342 [D loss: 0.888963, acc.: 42.19%] [G loss: 1.074312]\n",
      "8/8 [==============================] - 0s 49ms/step\n",
      "343 [D loss: 0.788640, acc.: 50.39%] [G loss: 1.178565]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "344 [D loss: 0.810498, acc.: 51.37%] [G loss: 1.179790]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "345 [D loss: 0.711512, acc.: 54.88%] [G loss: 1.151867]\n",
      "8/8 [==============================] - 0s 55ms/step\n",
      "346 [D loss: 0.660877, acc.: 60.94%] [G loss: 1.124954]\n",
      "8/8 [==============================] - 0s 52ms/step\n",
      "347 [D loss: 0.740029, acc.: 52.73%] [G loss: 0.953819]\n",
      "8/8 [==============================] - 0s 49ms/step\n",
      "348 [D loss: 0.625475, acc.: 64.84%] [G loss: 1.000155]\n",
      "8/8 [==============================] - 0s 53ms/step\n",
      "349 [D loss: 0.678137, acc.: 60.16%] [G loss: 1.042975]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "350 [D loss: 0.643381, acc.: 62.50%] [G loss: 0.840330]\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "351 [D loss: 0.650509, acc.: 63.28%] [G loss: 0.840142]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "352 [D loss: 0.566538, acc.: 73.44%] [G loss: 0.913759]\n",
      "8/8 [==============================] - 0s 55ms/step\n",
      "353 [D loss: 0.572791, acc.: 70.51%] [G loss: 0.894558]\n",
      "8/8 [==============================] - 0s 52ms/step\n",
      "354 [D loss: 0.610703, acc.: 67.97%] [G loss: 0.985957]\n",
      "8/8 [==============================] - 0s 50ms/step\n",
      "355 [D loss: 0.554449, acc.: 72.07%] [G loss: 1.012683]\n",
      "8/8 [==============================] - 0s 51ms/step\n",
      "356 [D loss: 0.590707, acc.: 68.16%] [G loss: 1.028180]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "357 [D loss: 0.554880, acc.: 74.22%] [G loss: 1.106022]\n",
      "8/8 [==============================] - 0s 51ms/step\n",
      "358 [D loss: 0.655503, acc.: 63.87%] [G loss: 1.094838]\n",
      "8/8 [==============================] - 0s 51ms/step\n",
      "359 [D loss: 0.776307, acc.: 52.15%] [G loss: 1.098166]\n",
      "8/8 [==============================] - 0s 49ms/step\n",
      "360 [D loss: 0.827105, acc.: 44.14%] [G loss: 1.174603]\n",
      "8/8 [==============================] - 0s 55ms/step\n",
      "361 [D loss: 0.730244, acc.: 56.05%] [G loss: 1.248585]\n",
      "8/8 [==============================] - 0s 56ms/step\n",
      "362 [D loss: 0.785756, acc.: 48.05%] [G loss: 1.213927]\n",
      "8/8 [==============================] - 0s 52ms/step\n",
      "363 [D loss: 0.731860, acc.: 53.52%] [G loss: 1.138750]\n",
      "8/8 [==============================] - 0s 59ms/step\n",
      "364 [D loss: 0.700855, acc.: 57.62%] [G loss: 1.123406]\n",
      "8/8 [==============================] - 0s 58ms/step\n",
      "365 [D loss: 0.836187, acc.: 45.31%] [G loss: 1.080523]\n",
      "8/8 [==============================] - 1s 60ms/step\n",
      "366 [D loss: 0.810792, acc.: 49.61%] [G loss: 1.073570]\n",
      "8/8 [==============================] - 0s 51ms/step\n",
      "367 [D loss: 0.753300, acc.: 54.49%] [G loss: 1.089905]\n",
      "8/8 [==============================] - 0s 52ms/step\n",
      "368 [D loss: 0.763281, acc.: 53.71%] [G loss: 1.022965]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "369 [D loss: 0.675467, acc.: 60.55%] [G loss: 0.964457]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "370 [D loss: 0.693560, acc.: 58.40%] [G loss: 0.971349]\n",
      "8/8 [==============================] - 0s 53ms/step\n",
      "371 [D loss: 0.681609, acc.: 56.84%] [G loss: 0.884842]\n",
      "8/8 [==============================] - 0s 49ms/step\n",
      "372 [D loss: 0.602690, acc.: 66.60%] [G loss: 0.963605]\n",
      "8/8 [==============================] - 0s 52ms/step\n",
      "373 [D loss: 0.640554, acc.: 62.11%] [G loss: 1.032377]\n",
      "8/8 [==============================] - 0s 52ms/step\n",
      "374 [D loss: 0.728575, acc.: 55.66%] [G loss: 1.008864]\n",
      "8/8 [==============================] - 0s 51ms/step\n",
      "375 [D loss: 0.623285, acc.: 64.65%] [G loss: 1.033700]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "376 [D loss: 0.796730, acc.: 50.20%] [G loss: 1.045983]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "377 [D loss: 0.741038, acc.: 52.15%] [G loss: 1.113084]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "378 [D loss: 0.739872, acc.: 54.88%] [G loss: 1.185495]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "379 [D loss: 0.725817, acc.: 57.42%] [G loss: 1.098012]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "380 [D loss: 0.676799, acc.: 58.40%] [G loss: 1.117231]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "381 [D loss: 0.673622, acc.: 59.77%] [G loss: 1.109271]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "382 [D loss: 0.744273, acc.: 55.47%] [G loss: 1.073149]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "383 [D loss: 0.692853, acc.: 59.38%] [G loss: 1.009427]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "384 [D loss: 0.697999, acc.: 56.45%] [G loss: 0.962980]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "385 [D loss: 0.781799, acc.: 48.44%] [G loss: 0.940784]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "386 [D loss: 0.770555, acc.: 51.56%] [G loss: 0.938239]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "387 [D loss: 0.640988, acc.: 62.11%] [G loss: 0.937172]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "388 [D loss: 0.643158, acc.: 64.26%] [G loss: 0.893189]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "389 [D loss: 0.561093, acc.: 71.09%] [G loss: 0.999083]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "390 [D loss: 0.554080, acc.: 72.85%] [G loss: 1.016796]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "391 [D loss: 0.696698, acc.: 57.81%] [G loss: 1.108293]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "392 [D loss: 0.721349, acc.: 55.86%] [G loss: 1.098781]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "393 [D loss: 0.775864, acc.: 50.20%] [G loss: 1.111683]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "394 [D loss: 0.846952, acc.: 45.51%] [G loss: 1.285758]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "395 [D loss: 0.867303, acc.: 43.95%] [G loss: 1.185971]\n",
      "8/8 [==============================] - 0s 53ms/step\n",
      "396 [D loss: 0.854805, acc.: 43.75%] [G loss: 1.267447]\n",
      "8/8 [==============================] - 0s 51ms/step\n",
      "397 [D loss: 0.697419, acc.: 59.18%] [G loss: 1.174958]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "398 [D loss: 0.788992, acc.: 47.46%] [G loss: 1.056366]\n",
      "8/8 [==============================] - 0s 51ms/step\n",
      "399 [D loss: 0.741618, acc.: 53.12%] [G loss: 1.083501]\n",
      "8/8 [==============================] - 0s 50ms/step\n",
      "400 [D loss: 0.828230, acc.: 47.07%] [G loss: 0.989300]\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "401 [D loss: 0.781039, acc.: 52.15%] [G loss: 0.986637]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 54ms/step\n",
      "402 [D loss: 0.633663, acc.: 62.89%] [G loss: 0.962584]\n",
      "8/8 [==============================] - 0s 51ms/step\n",
      "403 [D loss: 0.671896, acc.: 61.72%] [G loss: 0.940681]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "404 [D loss: 0.613630, acc.: 63.48%] [G loss: 0.955350]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "405 [D loss: 0.588292, acc.: 70.12%] [G loss: 0.985506]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "406 [D loss: 0.629769, acc.: 68.16%] [G loss: 1.054093]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "407 [D loss: 0.648262, acc.: 61.72%] [G loss: 0.997207]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "408 [D loss: 0.669293, acc.: 62.50%] [G loss: 1.054397]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "409 [D loss: 0.744486, acc.: 51.76%] [G loss: 1.074263]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "410 [D loss: 0.810752, acc.: 48.05%] [G loss: 1.116103]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "411 [D loss: 0.804482, acc.: 47.46%] [G loss: 1.121960]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "412 [D loss: 0.791618, acc.: 50.59%] [G loss: 1.068899]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "413 [D loss: 0.785441, acc.: 52.34%] [G loss: 1.098508]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "414 [D loss: 0.825176, acc.: 44.14%] [G loss: 1.109426]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "415 [D loss: 0.758816, acc.: 52.15%] [G loss: 1.057191]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "416 [D loss: 0.680940, acc.: 60.35%] [G loss: 0.998317]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "417 [D loss: 0.756885, acc.: 50.98%] [G loss: 0.953478]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "418 [D loss: 0.611657, acc.: 65.04%] [G loss: 0.968743]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "419 [D loss: 0.728956, acc.: 52.34%] [G loss: 0.959029]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "420 [D loss: 0.602099, acc.: 66.80%] [G loss: 0.973029]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "421 [D loss: 0.627361, acc.: 63.67%] [G loss: 0.927093]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "422 [D loss: 0.489294, acc.: 79.88%] [G loss: 0.924876]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "423 [D loss: 0.566311, acc.: 69.73%] [G loss: 0.909217]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "424 [D loss: 0.504649, acc.: 76.37%] [G loss: 1.000981]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "425 [D loss: 0.529956, acc.: 74.22%] [G loss: 1.044734]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "426 [D loss: 0.636435, acc.: 65.82%] [G loss: 0.966792]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "427 [D loss: 0.680809, acc.: 60.16%] [G loss: 0.995328]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "428 [D loss: 0.721309, acc.: 56.05%] [G loss: 1.041878]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "429 [D loss: 0.758435, acc.: 52.34%] [G loss: 1.050708]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "430 [D loss: 0.725438, acc.: 51.17%] [G loss: 1.023510]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "431 [D loss: 0.857242, acc.: 45.31%] [G loss: 1.030474]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "432 [D loss: 0.811396, acc.: 47.46%] [G loss: 1.111531]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "433 [D loss: 0.744780, acc.: 54.69%] [G loss: 1.112925]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "434 [D loss: 0.767454, acc.: 51.17%] [G loss: 1.044554]\n",
      "8/8 [==============================] - 0s 49ms/step\n",
      "435 [D loss: 0.759087, acc.: 52.34%] [G loss: 1.132499]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "436 [D loss: 0.635791, acc.: 66.02%] [G loss: 1.132494]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "437 [D loss: 0.697246, acc.: 59.18%] [G loss: 1.058910]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "438 [D loss: 0.615443, acc.: 69.14%] [G loss: 1.052286]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "439 [D loss: 0.599624, acc.: 67.19%] [G loss: 1.015300]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "440 [D loss: 0.477184, acc.: 79.10%] [G loss: 1.101436]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "441 [D loss: 0.473803, acc.: 79.88%] [G loss: 1.019009]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "442 [D loss: 0.472261, acc.: 78.71%] [G loss: 1.013573]\n",
      "8/8 [==============================] - 0s 52ms/step\n",
      "443 [D loss: 0.502898, acc.: 74.61%] [G loss: 0.996418]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "444 [D loss: 0.446348, acc.: 83.20%] [G loss: 0.923225]\n",
      "8/8 [==============================] - 0s 49ms/step\n",
      "445 [D loss: 0.535044, acc.: 74.22%] [G loss: 0.996964]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "446 [D loss: 0.457254, acc.: 79.30%] [G loss: 1.031184]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "447 [D loss: 0.698834, acc.: 58.59%] [G loss: 1.021196]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "448 [D loss: 0.638949, acc.: 63.48%] [G loss: 1.045230]\n",
      "8/8 [==============================] - 0s 53ms/step\n",
      "449 [D loss: 1.040180, acc.: 34.18%] [G loss: 1.064070]\n",
      "8/8 [==============================] - 0s 53ms/step\n",
      "450 [D loss: 0.791817, acc.: 47.07%] [G loss: 1.191711]\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "8/8 [==============================] - 0s 49ms/step\n",
      "451 [D loss: 0.914523, acc.: 37.11%] [G loss: 1.183470]\n",
      "8/8 [==============================] - 0s 53ms/step\n",
      "452 [D loss: 0.962702, acc.: 35.35%] [G loss: 1.178714]\n",
      "8/8 [==============================] - 0s 51ms/step\n",
      "453 [D loss: 0.878324, acc.: 41.60%] [G loss: 1.186794]\n",
      "8/8 [==============================] - 0s 49ms/step\n",
      "454 [D loss: 0.889135, acc.: 41.02%] [G loss: 1.219716]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "455 [D loss: 0.787024, acc.: 49.80%] [G loss: 1.168433]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "456 [D loss: 0.737420, acc.: 55.66%] [G loss: 1.165340]\n",
      "8/8 [==============================] - 0s 54ms/step\n",
      "457 [D loss: 0.734729, acc.: 54.69%] [G loss: 1.068022]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "458 [D loss: 0.635329, acc.: 63.87%] [G loss: 0.953263]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "459 [D loss: 0.658724, acc.: 61.33%] [G loss: 0.932129]\n",
      "8/8 [==============================] - 0s 51ms/step\n",
      "460 [D loss: 0.567030, acc.: 70.70%] [G loss: 0.896849]\n",
      "8/8 [==============================] - 0s 52ms/step\n",
      "461 [D loss: 0.517708, acc.: 74.22%] [G loss: 0.898814]\n",
      "8/8 [==============================] - 0s 56ms/step\n",
      "462 [D loss: 0.599387, acc.: 66.21%] [G loss: 0.928553]\n",
      "8/8 [==============================] - 0s 51ms/step\n",
      "463 [D loss: 0.812787, acc.: 49.61%] [G loss: 0.865110]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "464 [D loss: 0.726875, acc.: 56.45%] [G loss: 0.994931]\n",
      "8/8 [==============================] - 0s 52ms/step\n",
      "465 [D loss: 0.824128, acc.: 46.09%] [G loss: 1.007002]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "466 [D loss: 0.819269, acc.: 45.31%] [G loss: 1.057486]\n",
      "8/8 [==============================] - 0s 54ms/step\n",
      "467 [D loss: 0.922146, acc.: 35.94%] [G loss: 1.052992]\n",
      "8/8 [==============================] - 0s 49ms/step\n",
      "468 [D loss: 0.763149, acc.: 50.78%] [G loss: 1.036151]\n",
      "8/8 [==============================] - 0s 51ms/step\n",
      "469 [D loss: 0.805382, acc.: 47.66%] [G loss: 1.027081]\n",
      "8/8 [==============================] - 0s 50ms/step\n",
      "470 [D loss: 0.679560, acc.: 58.59%] [G loss: 1.034984]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "471 [D loss: 0.647646, acc.: 64.06%] [G loss: 0.973859]\n",
      "8/8 [==============================] - 0s 52ms/step\n",
      "472 [D loss: 0.562582, acc.: 72.85%] [G loss: 0.941174]\n",
      "8/8 [==============================] - 0s 50ms/step\n",
      "473 [D loss: 0.416772, acc.: 85.35%] [G loss: 0.920668]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "474 [D loss: 0.391127, acc.: 86.33%] [G loss: 0.899108]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "475 [D loss: 0.366894, acc.: 90.04%] [G loss: 0.908668]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "476 [D loss: 0.405180, acc.: 84.77%] [G loss: 0.933936]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 49ms/step\n",
      "477 [D loss: 0.382479, acc.: 86.52%] [G loss: 1.024582]\n",
      "8/8 [==============================] - 0s 48ms/step\n",
      "478 [D loss: 0.439193, acc.: 84.57%] [G loss: 0.974380]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "479 [D loss: 0.521278, acc.: 74.02%] [G loss: 1.089310]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "480 [D loss: 0.630230, acc.: 67.58%] [G loss: 1.076224]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "481 [D loss: 0.783776, acc.: 52.54%] [G loss: 1.052352]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "482 [D loss: 0.765750, acc.: 51.76%] [G loss: 1.162861]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "483 [D loss: 0.956390, acc.: 33.79%] [G loss: 1.157515]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "484 [D loss: 0.860321, acc.: 41.60%] [G loss: 1.156845]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "485 [D loss: 0.902311, acc.: 39.84%] [G loss: 1.107591]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "486 [D loss: 0.828520, acc.: 47.66%] [G loss: 1.086677]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "487 [D loss: 0.706773, acc.: 56.84%] [G loss: 1.114403]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "488 [D loss: 0.766190, acc.: 52.73%] [G loss: 1.079358]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "489 [D loss: 0.698594, acc.: 57.03%] [G loss: 1.084055]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "490 [D loss: 0.513856, acc.: 74.80%] [G loss: 1.042683]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "491 [D loss: 0.498517, acc.: 76.56%] [G loss: 0.987909]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "492 [D loss: 0.466751, acc.: 81.25%] [G loss: 0.991956]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "493 [D loss: 0.550102, acc.: 69.73%] [G loss: 0.939974]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "494 [D loss: 0.439083, acc.: 83.40%] [G loss: 0.986534]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "495 [D loss: 0.431354, acc.: 82.81%] [G loss: 0.986612]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "496 [D loss: 0.430496, acc.: 85.16%] [G loss: 1.026256]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "497 [D loss: 0.438768, acc.: 84.18%] [G loss: 1.013133]\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "498 [D loss: 0.776496, acc.: 51.56%] [G loss: 1.021842]\n",
      "8/8 [==============================] - 0s 47ms/step\n",
      "499 [D loss: 0.817781, acc.: 48.05%] [G loss: 1.034673]\n"
     ]
    }
   ],
   "source": [
    "dcgan = DCGAN(28,28,1)\n",
    "dcgan.train(epochs=500, batch_size=256, save_interval=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af848b65",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
